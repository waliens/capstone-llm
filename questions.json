{"items": [{"tags": ["dbt"], "owner": {"account_id": 20309608, "reputation": 725, "user_id": 14897019, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AEdFTp4OZMeeKWelZEs8Li6wXDKHVQzXTekMxiYnx5Oh=k-s256", "display_name": "Pierre-Alexandre", "link": "https://stackoverflow.com/users/14897019/pierre-alexandre"}, "is_answered": true, "view_count": 40631, "accepted_answer_id": 72799561, "answer_count": 3, "score": 18, "last_activity_date": 1711621053, "creation_date": 1656495908, "question_id": 72799237, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72799237/only-run-a-set-of-models", "title": "Only run a set of models?", "body": "<p>I started to migrate some of your transformations jobs to DBT. As you can see on the image bellow, there is usually 1 to 2 transformations before to have our final table (up to 5 transformations in some cases).</p>\n<p>What I am trying to achieve is to do dbt run only for a set on linked model. For instance, <code>sales_prediction</code> and <code>forecast</code>.  I am currently able to run either for everything with <code>dbt run o</code>r just speficif model using <code>dbt run --select model_name</code></p>\n<p><a href=\"https://i.sstatic.net/s4jl5.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/s4jl5.png\" alt=\"enter image description here\" /></a></p>\n"}, {"tags": ["postgresql", "dbt"], "owner": {"account_id": 18804422, "reputation": 171, "user_id": 13714082, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9bd56a8002be346b2f8969ba91e006fb?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Sara", "link": "https://stackoverflow.com/users/13714082/sara"}, "is_answered": true, "view_count": 15253, "answer_count": 1, "score": 16, "last_activity_date": 1691687274, "creation_date": 1663668003, "last_edit_date": 1691687274, "question_id": 73784913, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73784913/dbt-ref-vs-source", "title": "dbt ref() vs source()", "body": "<p>I\u2019m trying to make one model in dbt depend on another (trying to run the second model after the first one is completely finished), but I\u2019m confused, when to use <code>ref()</code> or <code>source()</code>?</p>\n<p>What is the difference between them?</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 9237891, "reputation": 170, "user_id": 6861925, "user_type": "registered", "profile_image": "https://graph.facebook.com/1709350879390046/picture?type=large", "display_name": "Willie Chen", "link": "https://stackoverflow.com/users/6861925/willie-chen"}, "is_answered": true, "view_count": 16756, "answer_count": 4, "score": 16, "last_activity_date": 1625265473, "creation_date": 1595271657, "last_edit_date": 1595790121, "question_id": 63002171, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63002171/can-dbt-connect-to-different-databases-in-the-same-project", "title": "Can dbt connect to different databases in the same project?", "body": "<p>It seems dbt only works for a single database.</p>\n<p>If my data is in a different database, will that still work? For example, if my datalake is using delta, but I want to run dbt using Redshift, would dbt still work for this case?</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 8495759, "reputation": 275, "user_id": 6370402, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/405d8de362cab93c0f8c40dfa1d95f96?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Catazza", "link": "https://stackoverflow.com/users/6370402/catazza"}, "is_answered": true, "view_count": 6009, "accepted_answer_id": 69849432, "answer_count": 1, "score": 16, "last_activity_date": 1636094292, "creation_date": 1634827884, "question_id": 69664170, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69664170/dbt-custom-schema-using-folder-structure", "title": "DBT custom schema using folder structure", "body": "<p>is there a way in DBT to create custom schemas for a model in a derived way by looking at the folder structure?</p>\n<p>For example, say this is my structure:</p>\n<pre><code>models\n\u2514-- product1\n    \u2514-- team1\n    |   \u2514-- model1.sql\n    \u2514-- team2\n        \u2514-- model2.sql\n</code></pre>\n<p>In this case, model1.sql would be created in the schema <code>product1_team1</code> whereas model2.sql would be created in the schema <code>product1_team2</code>. I guess I can specify those &quot;by hand&quot; in the <code>dbt_project.yml</code> file, but I was wondering if there was a way to do this in an automated way - so that every new model or folder is automatically created in the right schema.</p>\n<p>I was looking at custom schema macros (<a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas\" rel=\"noreferrer\">https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas</a>) but it seems to be plain jinja or simple Python built-ins. Not sure how I would be able to access folder paths in those macros.</p>\n<p>Also, is there a way to write a macro in Python? as it could be relatively straightforward knowing the file path and with the os module.</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 10701804, "reputation": 243, "user_id": 7876879, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ef2122792a49d44a16700b79ad4152eb?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Even", "link": "https://stackoverflow.com/users/7876879/even"}, "is_answered": true, "view_count": 13828, "accepted_answer_id": 69550357, "answer_count": 1, "score": 14, "last_activity_date": 1634104357, "creation_date": 1634103783, "question_id": 69550294, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69550294/how-to-setup-dbt-ui-for-data-lineage", "title": "How to setup dbt UI for data lineage?", "body": "<p>I'm new to the dbt and I'm planning to use dbt cli. One question is how to setup the dbt ui and have such a data lineage graph? I didn't find how to do it here with cli <a href=\"https://docs.getdbt.com/tutorial/create-a-project-dbt-cli\" rel=\"noreferrer\">https://docs.getdbt.com/tutorial/create-a-project-dbt-cli</a>.</p>\n<p><a href=\"https://i.sstatic.net/xrBa2.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/xrBa2.png\" alt=\"enter image description here\" /></a></p>\n"}, {"tags": ["sql", "postgresql", "dbt", "sqlfluff"], "owner": {"account_id": 9960840, "reputation": 882, "user_id": 7775043, "user_type": "registered", "profile_image": "https://i.sstatic.net/iAxC4.png?s=256", "display_name": "Albin", "link": "https://stackoverflow.com/users/7775043/albin"}, "is_answered": true, "view_count": 21501, "accepted_answer_id": 66915937, "answer_count": 3, "score": 14, "last_activity_date": 1697444301, "creation_date": 1617138133, "last_edit_date": 1671287230, "question_id": 66878455, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66878455/configure-sqlfluff-rules", "title": "Configure SQLFluff rules", "body": "<p>I use SQLFluff to ensure a uniform syntax in the company and reduce error warnings before running the models in dbt. Since our syntax does not completely match the syntax of SQLFluff I would like to make some changes.</p>\n<p>The <a href=\"https://docs.sqlfluff.com/en/stable/rules.html\" rel=\"noreferrer\">Rules References</a> provided by SQLFluff helped me to set up <em>Inline Ignoring Errors</em>, as displayed in the code below (last line of code).</p>\n<p>So I have two questions, which I was not able to answer also with the help of the Rules References of SQLFluff.</p>\n<ol>\n<li><p>I would like to set Rule <em>L032</em> as default 'false' without typing it manually every time in my SQL.</p>\n</li>\n<li><p>How do I change the maximal length of a line regarding Rule <em>L016</em>? I would like to set the default value e.g. 150.</p>\n</li>\n</ol>\n<p><a href=\"https://i.sstatic.net/byfQX.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/byfQX.png\" alt=\"Rule_L016\" /></a></p>\n<pre><code>SELECT\n    country.country_name,\n    country.population,\n    currency.currency_name,\n    currency.currency_id,\n    currency.strange_long_variable_name_which_is_too_long as not_so_long_variable_name\nFROM country\nLEFT JOIN currency\n    USING (country) -- noqa: L032\n</code></pre>\n<p>I tried to figure it out with the <a href=\"https://docs.sqlfluff.com/en/stable/rules.html\" rel=\"noreferrer\">Rules References</a> but could not figure it out. Help is very much appreciated!</p>\n"}, {"tags": ["sql", "dbt"], "owner": {"account_id": 16830498, "reputation": 384, "user_id": 12168607, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mB-3URtt_tRnF6Uo6kddeKtGlBx6JwHJdgyRwzBng=k-s256", "display_name": "Shahrukh lodhi", "link": "https://stackoverflow.com/users/12168607/shahrukh-lodhi"}, "is_answered": true, "view_count": 7699, "accepted_answer_id": 75019037, "answer_count": 1, "score": 14, "last_activity_date": 1672924690, "creation_date": 1632923231, "last_edit_date": 1633087557, "question_id": 69377677, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69377677/dbt-how-to-create-custom-table-without-using-ctas", "title": "dbt how to create custom table without using CTAS?", "body": "<p>I want to create an empty table with specific columns and data types, I don't have any reference table from where I can do SELECT * FROM . The following link has an image which I intend to do <a href=\"https://i.sstatic.net/fmJoo.png\" rel=\"noreferrer\">Please find the attached image</a></p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 11245218, "reputation": 2441, "user_id": 8248194, "user_type": "registered", "profile_image": "https://i.sstatic.net/9Y5b3.jpg?s=256", "display_name": "David Masip", "link": "https://stackoverflow.com/users/8248194/david-masip"}, "is_answered": true, "view_count": 10361, "accepted_answer_id": 62683107, "answer_count": 4, "score": 13, "last_activity_date": 1705011069, "creation_date": 1593602271, "last_edit_date": 1705011069, "question_id": 62675644, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62675644/write-dbt-test-for-positive-values", "title": "Write dbt test for positive values", "body": "<p>Is there an easy way to write a test for a column being positive in dbt?</p>\n<p><code>accepted_values</code> doesn't seem to work for continuous variables.</p>\n<p>I know you can write queries in <code>./tests</code> but it looks like an overkill for such a simple thing.</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 18944707, "reputation": 233, "user_id": 13823885, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb3eaba8002b3278e5d5e8a30c9ef52d?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Ravi", "link": "https://stackoverflow.com/users/13823885/ravi"}, "is_answered": true, "view_count": 52546, "answer_count": 5, "score": 13, "last_activity_date": 1665482976, "creation_date": 1593961248, "last_edit_date": 1593975106, "question_id": 62742369, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62742369/while-running-the-dbt-run-command-gets-error", "title": "while running the dbt run command gets error", "body": "<p>connection to dbt and snowfalke was successful but when tried to run this command:</p>\n<pre><code>$ dbt run\n</code></pre>\n<p>it gives this error</p>\n<blockquote>\n<p>ERROR: Runtime Error\nCould not find profile named 'learn_dbt'\nEncountered an error:\nRuntime Error\nCould not run dbt&quot;</p>\n</blockquote>\n<p>Am I making any command mistake?</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 19534633, "reputation": 191, "user_id": 14293665, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/71407596d8492a4bb1a630b1aaa3ce4b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Sweety", "link": "https://stackoverflow.com/users/14293665/sweety"}, "is_answered": true, "view_count": 29922, "answer_count": 4, "score": 11, "last_activity_date": 1677673031, "creation_date": 1600768336, "last_edit_date": 1677412122, "question_id": 64007239, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64007239/how-do-we-define-select-statement-as-a-variable-in-dbt", "title": "How do we define select statement as a variable in dbt?", "body": "<p>Hi I am trying to define a select statement in a set variable in dbt,\ncan any one suggest how to set sql query as a variable in dbt  and how to access those variables in below CTEs?</p>\n"}, {"tags": ["python-3.x", "windows", "cmd", "powershell-2.0", "dbt"], "owner": {"account_id": 5333471, "reputation": 581, "user_id": 4253365, "user_type": "registered", "profile_image": "https://i.sstatic.net/wvSzZ.jpg?s=256", "display_name": "Ali Hasan", "link": "https://stackoverflow.com/users/4253365/ali-hasan"}, "is_answered": true, "view_count": 55955, "answer_count": 11, "score": 11, "last_activity_date": 1711621134, "creation_date": 1575282296, "last_edit_date": 1595411127, "question_id": 59136919, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59136919/installed-dbt-but-getting-error-dbt-command-not-found-error", "title": "Installed dbt but getting error &quot;DBT command not found error&quot;", "body": "<p>I would like to use <code>dbt</code> (Data build tool) in one of my project. I am facing hurdle while creating a project or using DBT command. </p>\n\n<p>I have completed the installation process as described on DBT website given here: <a href=\"https://docs.getdbt.com/v0.10/docs/windows\" rel=\"noreferrer\">https://docs.getdbt.com/v0.10/docs/windows</a>. DBT installed successfully but when I tried to use DBT command for creating project it gave me error: </p>\n\n<blockquote>\n  <p>'dbt' is not recognized as an internal or external command, operable\n  program or batch file.</p>\n</blockquote>\n\n<p>I am using windows 10, and I have tried it for python 3.6 as well as python 3.7 version.</p>\n\n<p>any help would be highly appreciated!\nThanks</p>\n"}, {"tags": ["snowflake-cloud-data-platform", "dbt"], "owner": {"account_id": 2425955, "reputation": 2800, "user_id": 2117872, "user_type": "registered", "profile_image": "https://i.sstatic.net/QjcJX.jpg?s=256", "display_name": "David Garrison", "link": "https://stackoverflow.com/users/2117872/david-garrison"}, "is_answered": true, "view_count": 10857, "accepted_answer_id": 72344192, "answer_count": 2, "score": 11, "last_activity_date": 1705840767, "creation_date": 1653077869, "last_edit_date": 1653078545, "question_id": 72324405, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72324405/using-multiple-columns-in-a-unique-key-for-incremental-loading-in-dbt", "title": "Using multiple columns in a Unique_Key for incremental loading in DBT", "body": "<p>For incremental models, the DBT documentation <a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/building-models/configuring-incremental-models\" rel=\"noreferrer\">here</a> says:</p>\n<blockquote>\n<p>The unique_key should be supplied in your model definition as a string representing a simple column or a list of single quoted column names that can be used together, for example, ['col1', 'col2', \u2026])</p>\n</blockquote>\n<p>I've built an incremental model in DBT with this incremental definition</p>\n<pre><code>{{\n  config(\n    materialized='incremental',\n    unique_key = ['Col1', 'Col2', 'Col3']\n  )\n}}\n</code></pre>\n<p>Which compiles into this merge statement in in Snowflake:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>using DW_DEV.dbt_dgarrison_DATA_STAGING.MY_TABLE__dbt_tmp as DBT_INTERNAL_SOURCE\n    on \n        DBT_INTERNAL_SOURCE.['Col1', 'Col2', 'Col3'] = DBT_INTERNAL_DEST.['Col1', 'Col2', 'Col3']\n...\n</code></pre>\n<p>And this reasonably throws a SQL ERROR complaining about the brackets:</p>\n<blockquote>\n<p>SQL compilation error: syntax error line 4 at position 32 unexpected '['. syntax error line 4 at position 45 unexpected ','. syntax error line 4 at position 98 unexpected '['. syntax error line 4 at position 111 unexpected ','.</p>\n</blockquote>\n<p>I can't find any other good examples using multiple columns this way. (there are options involving concatenating columns, and I'm open to recommendations on the best approach to that, but I'm trying to figure out how to use the DBT recommended syntax)</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 9002381, "reputation": 1300, "user_id": 6710525, "user_type": "registered", "profile_image": "https://i.sstatic.net/4oLy6.jpg?s=256", "display_name": "totooooo", "link": "https://stackoverflow.com/users/6710525/totooooo"}, "is_answered": true, "view_count": 5388, "accepted_answer_id": 60567491, "answer_count": 1, "score": 11, "last_activity_date": 1624027277, "creation_date": 1583503549, "question_id": 60565688, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60565688/what-is-the-user-yml-file-what-is-its-purpose", "title": "What is the .user.yml file? What is its purpose?", "body": "<p>I'm starting with DBT. When I ran my project it created a <code>.user.yml</code> file. Its content:</p>\n\n<pre><code>{id: c8e8abd2-09a3-4699-b444-3ef7ee5b04e5}\n</code></pre>\n\n<p>It seems from <a href=\"https://github.com/fishtown-analytics/dbt/issues/1645\" rel=\"noreferrer\">this github issue</a> that it's some kind of cookie, but I could not find any info anywhere on what its role is.</p>\n\n<p>Can someone explain the purpose of this file? Should I add it to my <code>.gitignore</code>?</p>\n"}, {"tags": ["jinja2", "dbt"], "owner": {"account_id": 85788, "reputation": 1222, "user_id": 238968, "user_type": "registered", "accept_rate": 73, "profile_image": "https://www.gravatar.com/avatar/2634b26733c03daf1532b78b84e2be3b?s=256&d=identicon&r=PG", "display_name": "Ravi", "link": "https://stackoverflow.com/users/238968/ravi"}, "is_answered": true, "view_count": 39798, "accepted_answer_id": 65617106, "answer_count": 1, "score": 10, "last_activity_date": 1610040206, "creation_date": 1610029246, "last_edit_date": 1610029468, "question_id": 65614108, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65614108/using-if-block-in-dbt-models", "title": "Using if block in dbt models", "body": "<p>Apologies for asking dumb question. But I tried many different approaches but none of them seems to work.</p>\n<p>I have a requirement to select data from 2 different tables based on the variable. I am trying to do that in dbt models with if statement but it doesn't seem to work.</p>\n<p>Model looks something like thins:</p>\n<pre><code>SELECT \n*\nFROM\n{% if enable_whitelisting == 'true' %}\n    {{ ref('accounts_whitelisted') }}    accounts\n{% else %}\n        {{ ref('accounts') }}   accounts\n{% endif %}\n</code></pre>\n<p>Any help is appreciated.</p>\n<p>Thanks in advance.</p>\n"}, {"tags": ["snowflake-cloud-data-platform", "dbt"], "owner": {"account_id": 2344016, "reputation": 103, "user_id": 2054735, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ca6bb72d7a1ac7f012c62cb98e0177ca?s=256&d=identicon&r=PG", "display_name": "stuartcoggins", "link": "https://stackoverflow.com/users/2054735/stuartcoggins"}, "is_answered": true, "view_count": 5544, "accepted_answer_id": 69484724, "answer_count": 2, "score": 10, "last_activity_date": 1639417350, "creation_date": 1633620526, "last_edit_date": 1633620987, "question_id": 69483842, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69483842/why-do-i-get-a-select-active-warehouse-error-in-dbt-when-trying-the-table-mate", "title": "Why do I get a &#39;select active warehouse&#39; error in dbt when trying the table materialization, but not with the view materialization?", "body": "<p>I've been working with dbt for a couple of months now, so still fairly new to it. When running a test model, I have no problems when using the view materialization:</p>\n<pre><code>{{ config(materialized='view') }}\n\nselect 1 as id\n</code></pre>\n<p>Resulting in:</p>\n<pre><code>15:30:25 | 1 of 1 START view model dbt.stg_CampaignTableTest.................... [RUN]\n15:30:26 | 1 of 1 OK created view model dbt.stg_CampaignTableTest............... [SUCCESS 1 in 1.48s]\n</code></pre>\n<p>However, when I make the switch to a table materialization I get an error message about not having an active warehouse selected in Snowflake:</p>\n<pre><code>{{ config(materialized='table') }}\n\nselect 1 as id\n</code></pre>\n<p>Resulting in:</p>\n<pre><code>15:32:52 | 1 of 1 START table model dbt.stg_CampaignTableTest................... [RUN]\n15:32:53 | 1 of 1 ERROR creating table model dbt.stg_CampaignTableTest.......... [ERROR in 1.22s]\n\nDatabase Error in model stg_CampaignTableTest (models/test/stg_CampaignTableTest.sql)\n  000606 (57P03): No active warehouse selected in the current session.  Select an active warehouse with the 'use warehouse' command.\n</code></pre>\n<p>Of course, it's not possible to include a &quot;use warehouse&quot; statement within my test model as it is inserted into the compiled SQL at the wrong position:</p>\n<pre><code>{{ config(materialized='table') }}\n\nuse warehouse &quot;AnalysisTeam_WH&quot;;\n\nselect 1 as id\n</code></pre>\n<p>Because it leads to:</p>\n<pre><code>2021-10-07T15:33:59.366279Z: On model.my_new_project.stg_CampaignTableTest: /* {&quot;app&quot;: &quot;dbt&quot;, &quot;dbt_version&quot;: &quot;0.21.0&quot;, &quot;profile_name&quot;: &quot;user&quot;, &quot;target_name&quot;: &quot;default&quot;, &quot;node_id&quot;: &quot;model.my_new_project.stg_CampaignTableTest&quot;} */\n\n\n      create or replace transient table &quot;AnalysisTeam&quot;.&quot;dbt&quot;.&quot;stg_CampaignTableTest&quot;  as\n      (\n\nuse warehouse &quot;AnalysisTeam_WH&quot;;\n2021-10-07T15:33:59.366342Z: Opening a new connection, currently in state closed\n2021-10-07T15:34:00.163673Z: Snowflake query id: 019f7386-3200-ec67-0000-464100e189fa\n2021-10-07T15:34:00.163803Z: Snowflake error: 001003 (42000): SQL compilation error:\nsyntax error line 4 at position 0 unexpected 'use'.\n</code></pre>\n<p>I appear to have the correct permissions with my Snowflake 'role' to create tables, views, etc., so I was at a loss to understand why changing from view to table would cause the model to fail. I suspect it could be related to Snowflake permissions rather than a dbt issue but I am not sure. Any ideas would be really appreciated!</p>\n<p>Edit: I appeared to make a mistake with my screenshots so I have switched to code snippets which is hopefully clearer.</p>\n"}, {"tags": ["snowflake-cloud-data-platform", "dbt"], "owner": {"account_id": 26289587, "reputation": 101, "user_id": 19956796, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b85751dab32676d3d656e02175323030?s=256&d=identicon&r=PG", "display_name": "paul1315", "link": "https://stackoverflow.com/users/19956796/paul1315"}, "is_answered": true, "view_count": 2167, "answer_count": 1, "score": 10, "last_activity_date": 1696699879, "creation_date": 1662718421, "question_id": 73660554, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73660554/can-dbt-cloud-be-setup-to-use-mfa-when-connecting-to-snowflake", "title": "Can DBT Cloud be setup to use MFA when connecting to Snowflake?", "body": "<p>I have a Snowflake account that uses MFA.\nLogging in to Snowflake promtps for the MFA and I receive a push notification on my phone. Approving that logs me in.\nWhen setting up DBT Cloud it asks for details of the Snowflake account and then tries to test this. The cursor changes to the red circle / line and stays like that for a while before going back to normal and allowing the test to be run again. The Continue option remains greyed out.\nI expect the connection is timing out because Snowflake is waiting on the MFA to be approved, but as there is no notification in Duo there is nothing to approve.\nDBT docs have details of how to setup MFA for CLI but I can't see anything for DBT Cloud.\nIs MFA supported in DBT Cloud connections to Snowflake or should I just have a special Snowflake user that doesn't require MFA?</p>\n"}, {"tags": ["dbt", "confluence"], "owner": {"account_id": 21400147, "reputation": 111, "user_id": 15762465, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3dc822d5c70b0cc7467fad1d2499838c?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "rakimo", "link": "https://stackoverflow.com/users/15762465/rakimo"}, "is_answered": true, "view_count": 1198, "answer_count": 1, "score": 10, "last_activity_date": 1711136732, "creation_date": 1695538402, "question_id": 77166007, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/77166007/is-there-a-way-to-embed-dbt-docs-within-confluence", "title": "Is there a way to embed dbt docs within Confluence?", "body": "<p>dbt's built in docs is amazing for tracking data lineage, data dictionaries, business rules etc. But publishing these docs to a central place in a secure way without using dbt Cloud is proving to be not as straight forward.</p>\n<p>Like a lot of businesses we use Confluence for project tracking and documentation. It would be beneficial to have all documentation centralised in one place for both technical and business users. Has anyone embeded dbt docs within Confluence before? Are there any existing plug-ins for this?</p>\n<p>The nirvana state would be to have some CI which automatically keeps the embeded dbt docs up to date, but baby steps.</p>\n<p>I found this on the Confluence support page which talks about using Confluence to serve static content:\n<a href=\"https://confluence.atlassian.com/confkb/how-to-use-confluence-to-serve-static-content-677282407.html\" rel=\"noreferrer\">https://confluence.atlassian.com/confkb/how-to-use-confluence-to-serve-static-content-677282407.html</a></p>\n<p>But I'd prefer to not muck around with backend Confluence server things. It's a bit outside of my area of expertise.</p>\n"}, {"tags": ["environment-variables", "dbt"], "owner": {"account_id": 3294496, "reputation": 175, "user_id": 2772056, "user_type": "registered", "accept_rate": 50, "profile_image": "https://www.gravatar.com/avatar/5c7f1eb211d07507008cb591e0f338f4?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "user2772056", "link": "https://stackoverflow.com/users/2772056/user2772056"}, "is_answered": true, "view_count": 36612, "answer_count": 1, "score": 9, "last_activity_date": 1721156507, "creation_date": 1657646161, "last_edit_date": 1663208221, "question_id": 72956095, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72956095/dbt-environment-variables-and-running-dbt", "title": "DBT - environment variables and running dbt", "body": "<p>I am relatively new to DBT and I have been reading about <code>env_var</code> and I want to use this in a couple of situations and I am having difficultly and looking for some support.</p>\n<p>firstly I am trying to use it in my profiles.yml file to replace the user and password so that this can be set when it is invoked. When trying to test this locally (before implementing this on our AWS side) I am failing to find the right syntax and not finding anything useful online.\nI have tried variations of:</p>\n<pre><code>dbt run --vars '{DBT_USER: my_username, DBT_PASSWORD=my_password}'\n</code></pre>\n<p>but it is not recognizing and giving nothing useful error wise. When running dbt run by itself it does ask for <code>DBT_USER</code> so it is expecting it, but doesn't detail how</p>\n<p>I would also like to use it in my <code>dbt_project.yml</code> for the schema but I assume that this will be similar to the above, just a third variable at the end. Is that the case?</p>\n<p>Thanks</p>\n"}, {"tags": ["snowflake-cloud-data-platform", "dbt"], "owner": {"account_id": 19801591, "reputation": 113, "user_id": 14502473, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-w6sAaksslHE/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckQBLTiOR4lRsUtIrvUWCIJ0RLVkQ/s96-c/photo.jpg?sz=256", "display_name": "Kyle Cheung", "link": "https://stackoverflow.com/users/14502473/kyle-cheung"}, "is_answered": true, "view_count": 22665, "accepted_answer_id": 64490072, "answer_count": 2, "score": 9, "last_activity_date": 1634713126, "creation_date": 1603397149, "question_id": 64489772, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64489772/materialized-view-vs-table-using-dbt", "title": "Materialized View vs Table Using dbt", "body": "<p>I'm just onboarding dbt and having gone through the tutorial docs I'm wondering if there's a difference between materializing my transformations as views or tables? I'm using Snowflake as the data warehouse. There's some documentation <a href=\"https://docs.snowflake.com/en/user-guide/views-materialized.html#comparison-with-tables-regular-views-and-cached-results\" rel=\"noreferrer\">here</a> that shows the differences between a table and a materialized view but if I'm using dbt to update the tables regularly, do they more or less become the same thing?</p>\n<p>Thanks!</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 23928594, "reputation": 91, "user_id": 17923414, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/49d42baf9efb76ec3c8bb90c41b3e1b8?s=256&d=identicon&r=PG", "display_name": "fmbcooney", "link": "https://stackoverflow.com/users/17923414/fmbcooney"}, "is_answered": true, "view_count": 13936, "answer_count": 4, "score": 9, "last_activity_date": 1662109987, "creation_date": 1642070148, "question_id": 70695142, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70695142/dbt-warning-did-not-find-matching-node-for-patch", "title": "DBT - [WARNING]: Did not find matching node for patch", "body": "<p>I keep getting the error below when I use <em>dbt run</em> - I can't find anything on why this error occurs or how to fix it within the dbt documentation.</p>\n<pre><code>[WARNING]: Did not find matching node for patch with name 'vGenericView' in the 'models' section of file 'models\\generic_schema\\schema.sql'\n</code></pre>\n"}, {"tags": ["python", "dbt"], "owner": {"account_id": 21871, "reputation": 12069, "user_id": 53491, "user_type": "registered", "accept_rate": 56, "profile_image": "https://i.sstatic.net/Y3f3O.jpg?s=256", "display_name": "Brian Postow", "link": "https://stackoverflow.com/users/53491/brian-postow"}, "is_answered": true, "view_count": 14291, "accepted_answer_id": 75112535, "answer_count": 1, "score": 9, "last_activity_date": 1682610130, "creation_date": 1673623710, "last_edit_date": 1682610130, "question_id": 75111217, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75111217/how-do-i-run-dbt-models-from-a-python-script-or-program", "title": "How do I run DBT models from a Python script or program?", "body": "<p>I have a DBT project, and a python script will be grabbing data from the postgresql to produce output.</p>\n<p>However, part of the python script will need to make the DBT run. I haven't found the library that will let me cause a DBT run from an external script, but I'm pretty sure it exists. How do I do this?</p>\n<p>ETA: The correct answer may be to download the DBT CLI and then use python system calls to use that.... I was hoping for a library, but I'll take what I can get.</p>\n"}, {"tags": ["jinja2", "dbt"], "owner": {"account_id": 14897420, "reputation": 381, "user_id": 10757281, "user_type": "registered", "profile_image": "https://graph.facebook.com/10215776085348685/picture?type=large", "display_name": "Reid Williams", "link": "https://stackoverflow.com/users/10757281/reid-williams"}, "is_answered": true, "view_count": 9595, "answer_count": 2, "score": 9, "last_activity_date": 1704212248, "creation_date": 1630954445, "last_edit_date": 1704212248, "question_id": 69079158, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69079158/can-dbt-macros-accept-other-macros-as-arguments", "title": "Can dbt macros accept other macros as arguments?", "body": "<p>I am curious if I can pass macros into another macro like this:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>{% macro my_macro(a, b, another_macro) %}\n  ...\n  {{ another_macro(a,b) }}\n  ...\n{% endmacro %}\n</code></pre>\n<p><strong>BONUS:</strong>\nIf dbt's framework can allow it am able to how can I pass arguments to it?</p>\n<p>In R it would look like</p>\n<pre><code>my_callable_function &lt;- function(another_function, ...) {\n  another_function(...)\n}\n</code></pre>\n"}, {"tags": ["dbt"], "owner": {"account_id": 17236657, "reputation": 213, "user_id": 12479987, "user_type": "registered", "profile_image": "https://i.sstatic.net/VEJoa.jpg?s=256", "display_name": "Mincho", "link": "https://stackoverflow.com/users/12479987/mincho"}, "is_answered": true, "view_count": 13170, "accepted_answer_id": 65395870, "answer_count": 1, "score": 9, "last_activity_date": 1645714310, "creation_date": 1608564032, "question_id": 65395382, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65395382/how-to-run-multi-tag-selector", "title": "How to run multi-tag selector", "body": "<p>I am using dbt 0.18.1 and I follow the documentation about tags however I am curious to know how to run multi-tag selector as arguments.\nAccording to this:\n<a href=\"https://github.com/fishtown-analytics/dbt/pull/1014\" rel=\"noreferrer\">https://github.com/fishtown-analytics/dbt/pull/1014</a></p>\n<blockquote>\n<p>Select using a mix of tags, fqns, and parent/child selectors:\n$ dbt run --model tag:nightly+ salesforce.*+</p>\n</blockquote>\n<p>Unfortunately this is not really a &quot;mix of tags&quot;. <br/><br/>\nI have tags of [mixpanel_tests, quality] and I wish to run models that have both tags included (not separated). If I run <code>dbt run -m tag:quality -t blabla</code></p>\n<ol>\n<li>I would have executed all models that have QUALITY in the array of tags regardless if its single argument or multiple argument however I wish to run ONLY quality marked. How to do that?</li>\n<li>How do I specify 2 tags or 3 tags selector to run models with the mentioned tags (i.e mixpanel_tests, quality - but only those models that have both tags defined). More or less an AND clause rather than an OR clause.\n<br/>\nHmm I hope it is clear. How to have multitag selector that executes only the combination of tags given?</li>\n</ol>\n"}, {"tags": ["dbt"], "owner": {"account_id": 7766729, "reputation": 1081, "user_id": 5877690, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bf9430fe081fbefd6a40465fef651ce8?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Simon", "link": "https://stackoverflow.com/users/5877690/simon"}, "is_answered": true, "view_count": 2081, "accepted_answer_id": 70383575, "answer_count": 1, "score": 9, "last_activity_date": 1639687109, "creation_date": 1639674610, "question_id": 70382990, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70382990/configuring-raw-and-analytics-databases-with-dbt", "title": "Configuring raw and analytics databases with dbt", "body": "<p>I have been reading dbt's <a href=\"https://blog.getdbt.com/how-we-configure-snowflake/\" rel=\"noreferrer\">How we configure Snowflake</a> guide which explains the rationale behind having a <code>raw</code> database and an <code>analytics</code> database. Raw data is loaded into your warehouse into <code>raw</code> (e.g. by using Fivetran) and <code>analytics</code> is used by dbt to save transformed data/views for data analysts/scientists.</p>\n<p>However, I can't seem to find any guides on how to actually set this up. The profiles.yml file needs to point to where the raw data is, so that dbt can begin transforming. However, this file also seems to dictate the database and schema into which transformed data/views are saved.</p>\n<p>Where in dbt's many .yml files do I specify globally where to save transformed data?</p>\n"}, {"tags": ["python", "database", "gitlab", "dbt"], "owner": {"account_id": 21188503, "reputation": 139, "user_id": 15581402, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ee09bcfdfc113b3e38b754ed61bd9df3?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "zafar", "link": "https://stackoverflow.com/users/15581402/zafar"}, "is_answered": true, "view_count": 10632, "answer_count": 1, "score": 9, "last_activity_date": 1617907594, "creation_date": 1617872919, "question_id": 67000794, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67000794/how-can-we-work-with-multiple-dbt-projects", "title": "How can we work with multiple DBT projects", "body": "<p>can anyone share how to organise multiple projects in dbt , given the best practices. My present  abstract architecture hierarchy is as follows</p>\n<pre><code>Analytics \n--.dbt/\n-----profiles.yml\n--projects/\n-----project_1/\n----------models/\n----------dbt_project.yml\n-----project_2/\n----------models/\n----------dbt_project.yml\n--tests/\n-----projects/\n----------project_1/\n----------project_2/\n</code></pre>\n<p>To the point of creating models for either projects_1 or project_2 is working perfectly fine.</p>\n<p>but the problem comes when i try to run the test (model unit tests) for project_2.when I run it gives the error\n' no dbt_project.yml found at expected path in temp/project_1/ ...  ' (the unit tests that i am trying to run is for project_2)\nhowever, the paths are absolutely correct but the lookup for dbt_project.yml is in wrong directory (in the temp directory). On the side note, some of the project_2 models does depends on few project_1 models.</p>\n<p>can anyone share or reference where i can get the help to solve this sort of multi dbt project issue.</p>\n"}, {"tags": ["snowflake-task", "dbt"], "owner": {"account_id": 18944707, "reputation": 233, "user_id": 13823885, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb3eaba8002b3278e5d5e8a30c9ef52d?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Ravi", "link": "https://stackoverflow.com/users/13823885/ravi"}, "is_answered": true, "view_count": 13718, "answer_count": 5, "score": 8, "last_activity_date": 1695757621, "creation_date": 1593762402, "last_edit_date": 1593808231, "question_id": 62710809, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62710809/dbt-to-snowflake-connections-fails-via-profiles-yml", "title": "dbt to snowflake connections fails via profiles.yml", "body": "<p>I'm trying to connect to snowflake via dbt but connections fail with the error below:</p>\n<pre><code>Using profiles.yml file at /home/myname/.dbt/profiles.yml\nUsing dbt_project.yml file at /mnt/c/Users/Public/learn_dbt/rks-learn-dbt/learn_dbt/dbt_project.yml\nConfiguration:\n  profiles.yml file [ERROR invalid]\n  dbt_project.yml file [OK found and valid]\nProfile loading failed for the following reason:\nRuntime Error\n  Could not find profile named 'learn_dbt'\nRequired dependencies:\n - git [OK found] \n</code></pre>\n<p>Any advice please.</p>\n<p>Note: I am learning to setup dbt connections looking at udemy videos.</p>\n<p>Below is my <code>profiles.yml</code> file:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>learn_dbt:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: XXXXXX\n      user: XXXX                \n      password: XXXX                     \n      role: transform_role\n      database: analytics\n      warehouse: transform_wh\n      schema: dbt\n      threads: 1\n      client_session_keep_alive: False\n</code></pre>\n"}, {"tags": ["docker", "airflow", "dbt"], "owner": {"account_id": 15622610, "reputation": 2266, "user_id": 11271048, "user_type": "registered", "profile_image": "https://i.sstatic.net/8thDR.jpg?s=256", "display_name": "alt-f4", "link": "https://stackoverflow.com/users/11271048/alt-f4"}, "is_answered": true, "view_count": 3803, "accepted_answer_id": 65465102, "answer_count": 1, "score": 8, "last_activity_date": 1609071215, "creation_date": 1609064540, "last_edit_date": 1609071215, "question_id": 65464756, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65464756/running-dbt-within-airflow-through-the-docker-operator", "title": "Running DBT within Airflow through the Docker Operator", "body": "<p>Building my question on <a href=\"https://stackoverflow.com/questions/64890144/how-to-run-dbt-in-airflow-without-copying-our-repo\">How to run DBT in airflow without copying our repo</a>, I am currently running airflow and syncing the dags via git. I am considering different option to include DBT within my workflow. One suggestion by <a href=\"https://stackoverflow.com/users/3823815/louis-guitton\">louis_guitton</a> is to Dockerize the DBT project, and run it in Airflow via the <a href=\"https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html\" rel=\"noreferrer\">Docker Operator</a>.</p>\n<p>I have no prior experience using the Docker Operator in Airflow or generally DBT. I am wondering if anyone has tried or can provide some insights about their experience incorporating that workflow, my main questions are:</p>\n<ol>\n<li>Should DBT as a whole project be run as one Docker container, or is it broken down? (for example: are tests ran as a separate container from dbt tasks?)</li>\n<li>Are logs and the UI from DBT accessible and/or still useful when run via the Docker Operator?</li>\n<li>How would partial pipelines be run? (example: wanting to run only a part of the pipeline)</li>\n</ol>\n"}, {"tags": ["amazon-web-services", "airflow", "dbt", "mwaa"], "owner": {"account_id": 7563927, "reputation": 387, "user_id": 8569490, "user_type": "registered", "accept_rate": 67, "profile_image": "https://www.gravatar.com/avatar/8c410c41dfe1d3f94b14736fc8c63d0b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "nariver1", "link": "https://stackoverflow.com/users/8569490/nariver1"}, "is_answered": true, "view_count": 6564, "answer_count": 4, "score": 8, "last_activity_date": 1656725573, "creation_date": 1622815004, "question_id": 67838522, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67838522/how-to-use-dbt-with-aws-managed-airflow", "title": "How to use DBT with AWS Managed Airflow?", "body": "<p>hope you are doing well.\nI wanted to check if anyone has get up and running with dbt in aws mwaa airflow.</p>\n<p>I have tried without success <a href=\"https://github.com/gocardless/airflow-dbt\" rel=\"noreferrer\">this one</a> and <a href=\"https://github.com/tomasfarias/airflow-dbt-python\" rel=\"noreferrer\">this</a> python packages but fails for some reason or another (can't find the dbt path, etc).</p>\n<p>Did anyone has managed to use MWAA (Airflow 2) and DBT without having to build a docker image and placing it somewhere?</p>\n<p>Thank you!</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 17864589, "reputation": 870, "user_id": 12977233, "user_type": "registered", "profile_image": "https://i.sstatic.net/uT6rh.jpg?s=256", "display_name": "Shabari nath k", "link": "https://stackoverflow.com/users/12977233/shabari-nath-k"}, "is_answered": true, "view_count": 6003, "answer_count": 2, "score": 8, "last_activity_date": 1676864117, "creation_date": 1654715308, "question_id": 72550991, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72550991/how-is-the-usage-of-dbt-better-than-using-stored-procedure-method", "title": "How is the usage of DBT better than using stored procedure method?", "body": "<p>I have been using <code>stored procedure</code> method for a long time now.</p>\n<p>As a part of sales report generation, I create <code>stored procedures</code> to <code>join/union all</code> mulitple tables in database  and call it using <code>python</code> whenever i need it.</p>\n<p>Now <code>DBT</code> is a hot topic these days.</p>\n<p>Whats the advantage of moving to <code>DBT</code> from <code>stored procedures</code> ?\nIs there any point in migrating my entire <code>stored procedure</code> stack from stored proc to <code>DBT</code>?</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 8495759, "reputation": 275, "user_id": 6370402, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/405d8de362cab93c0f8c40dfa1d95f96?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Catazza", "link": "https://stackoverflow.com/users/6370402/catazza"}, "is_answered": true, "view_count": 9638, "answer_count": 4, "score": 8, "last_activity_date": 1668793255, "creation_date": 1638789319, "question_id": 70244607, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70244607/overriding-source-macro-in-dbt-to-allow-for-dynamic-sources-for-test-runs", "title": "Overriding source macro in DBT to allow for dynamic sources for test runs", "body": "<p>My aim is to be able to have &quot;dynamic&quot; sources depending on the type of DBT run I am doing. To be more precise, I am trying to find a solution to perform end-to-end business testing of our DBT models. I don't mean schema or simple data tests, but business logic tests. Something like, I have some input tables with test data, I run the DBT models and then I assert the end tables contain the desired results. I can create all the target tables in a separate schema by using a different 'test' profile, but I still need to be able to select from a different set of sources, which would be the test tables I am creating with the test data.</p>\n<p>I guess I can use jinja in the source files in combination with some variables to make this happen, but I am wondering if there is an even better way where I can do it without changing the source files at all. Like, the developers would not have to worry about writing code that also works for the tests. For this purpose, I was wondering if we can override the source macro, or do something along these lines, to incorporate this behaviour - similar to when we override the <code>generate_schema_name</code> macro. Something along the lines of (in python pseudocode):</p>\n<pre><code>def source(schema_name, table_name):\n    if env('is_test') == true:\n        return schema_name + table_name + '_test'\n    else:\n        return schema_name + table_name\n</code></pre>\n<p>I guess the complexity here is also that the source macro does more than that, for example sets some info for the lineage for the docs, which I definitely would like to keep.</p>\n<p>Any suggestion outside of this method are more than welcome!</p>\n"}, {"tags": ["airflow", "dbt"], "owner": {"account_id": 7315486, "reputation": 9973, "user_id": 5573294, "user_type": "registered", "accept_rate": 68, "profile_image": "https://i.sstatic.net/f7wtO.jpg?s=256", "display_name": "Canovice", "link": "https://stackoverflow.com/users/5573294/canovice"}, "is_answered": true, "view_count": 4309, "accepted_answer_id": 64890419, "answer_count": 3, "score": 7, "last_activity_date": 1686245991, "creation_date": 1605691112, "last_edit_date": 1686245935, "question_id": 64890144, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64890144/how-to-run-dbt-in-airflow-without-copying-our-repo", "title": "How to run DBT in airflow without copying our repo", "body": "<p>We use airflow to orchestrate our workflows, and dbt with bigquery for our daily transformations in BigQuery. We have two separate git repos, one for our dbt project and a separate one for airflow.</p>\n<p>It seems the simplest approach to scheduling our daily <code>run dbt</code> seems to be a <code>BashOperator</code> in airflow. However, to schedule DBT to run with Airflow, it seems like our entire DBT project would need to be nested inside of our Airflow project, that way we can point to it for our <code>dbt run</code> bash command?</p>\n<p>Is it possible to trigger our <code>dbt run</code> and <code>dbt test</code> without moving our DBT directory inside of our Airflow directory? With the <a href=\"https://github.com/gocardless/airflow-dbt\" rel=\"nofollow noreferrer\">airflow-dbt package</a>,  for the <code>dir</code> in the <code>default_args</code>, maybe it is possible to point to the gibhub link for the DBT project here?</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 19121362, "reputation": 91, "user_id": 13965268, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GivskPRmZS6iPki9mC2CB2NjopK-23cP1_paSPGqQ=k-s256", "display_name": "Niks A", "link": "https://stackoverflow.com/users/13965268/niks-a"}, "is_answered": true, "view_count": 12448, "answer_count": 3, "score": 7, "last_activity_date": 1595521139, "creation_date": 1595270501, "last_edit_date": 1595486921, "question_id": 63001872, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63001872/will-dbt-support-temp-table-creation-like-create-table-temp1-as-select-from-t", "title": "will DBT support temp table creation like create table #temp1 as select * from tab1 or it works only CTE way", "body": "<p>I found out a way to handle the temp tables in DBT, write all those in pre-hook and call the final temp table in the outside of the pre-hook, tested and is working fine, able to reduce the code running time from more than 20 mins to 1 min. But I see one problem that we can't see the lineage graph in the DBT documents.\nIs there any way to handle the temp tables other than pre-hook and with lineage in Docs?</p>\n"}, {"tags": ["sql", "dbt"], "owner": {"account_id": 16448714, "reputation": 83, "user_id": 11882668, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-eLYh2olrS3c/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdfDB4DS2ZnGhYUhDdcPCdLDQq0Vg/photo.jpg?sz=256", "display_name": "Nripesh Pradhan", "link": "https://stackoverflow.com/users/11882668/nripesh-pradhan"}, "is_answered": true, "view_count": 9181, "accepted_answer_id": 69445792, "answer_count": 3, "score": 7, "last_activity_date": 1714127147, "creation_date": 1633369871, "question_id": 69440332, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69440332/how-do-i-loop-through-alll-columns-using-jinja-in-dbt", "title": "How do I loop through alll columns using Jinja in DBT?", "body": "<p>I want to iterate over all the columns using dbt.</p>\n"}, {"tags": ["dbt", "meltano"], "owner": {"account_id": 4187455, "reputation": 2490, "user_id": 4298208, "user_type": "registered", "profile_image": "https://graph.facebook.com/598481447/picture?type=large", "display_name": "aaronsteers", "link": "https://stackoverflow.com/users/4298208/aaronsteers"}, "is_answered": true, "view_count": 1786, "accepted_answer_id": 70383829, "answer_count": 1, "score": 7, "last_activity_date": 1640716446, "creation_date": 1639675242, "question_id": 70383118, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70383118/using-dbt-and-meltano-how-can-i-prevent-multiple-dbt-job-runs-from-conflicting", "title": "Using dbt and Meltano, how can I prevent multiple dbt job runs from conflicting with each other?", "body": "<p>When running dbt jobs in Meltano, <code>dbt run</code> jobs may collide with each other if run out of a triggered context - for instance, when an on-demand job collides with a scheduled job or a CI-based job.</p>\n<p>If <code>dbt run</code> operates on the same tables at the same time, this generally causes a crash and sometimes a data quality issue if the same insert is performed twice on a single target table.</p>\n<p>Any way to prevent run collisions, using either Meltano functionality or native dbt functionality?</p>\n"}, {"tags": ["python", "dbt"], "owner": {"account_id": 1414093, "reputation": 3422, "user_id": 1340097, "user_type": "registered", "accept_rate": 70, "profile_image": "https://www.gravatar.com/avatar/5d9279085a5dc48b6f9ee14c4bacc0f7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "s_curry_s", "link": "https://stackoverflow.com/users/1340097/s-curry-s"}, "is_answered": true, "view_count": 8276, "accepted_answer_id": 70071661, "answer_count": 2, "score": 7, "last_activity_date": 1723563378, "creation_date": 1636570759, "last_edit_date": 1636571321, "question_id": 69918818, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69918818/how-can-i-run-python-code-after-a-dbt-run-or-a-specific-model-is-completed", "title": "How can I run python code after a DBT run (or a specific model) is completed?", "body": "<p>I would like to be able to run an ad-hoc python script that would access and run analytics on the model calculated by a dbt run, are there any best practices around this?</p>\n"}, {"tags": ["apache-spark", "hive", "parquet", "dbt"], "owner": {"account_id": 3910809, "reputation": 1798, "user_id": 3236215, "user_type": "registered", "accept_rate": 100, "profile_image": "https://i.sstatic.net/wzVbH.jpg?s=256", "display_name": "IVR", "link": "https://stackoverflow.com/users/3236215/ivr"}, "is_answered": true, "view_count": 7854, "accepted_answer_id": 63493307, "answer_count": 2, "score": 7, "last_activity_date": 1597951706, "creation_date": 1597852355, "last_edit_date": 1597906759, "question_id": 63490730, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63490730/using-external-parquet-tables-in-a-dbt-pipeline", "title": "Using external parquet tables in a DBT pipeline", "body": "<p>I'm trying to set up a simple DBT pipeline that uses a parquet tables stored on Azure Data Lake Storage and creates another tables that is also going to be stored in the same location.</p>\n<p>Under my <code>models/</code> (which is defined as my sources path) I have 2 files <code>datalake.yml</code> and <code>orders.sql</code>. <code>datalake.yml</code> looks like this:</p>\n<pre><code>version:2\nsources:\n   - name: datalake\n     tables:\n        - name: customers\n          external:\n             location: path/to/storage1 # I got this by from file properties in Azure\n             file_format: parquet\n          columns:\n             - name: id\n               data_type: int\n               description: &quot;ID&quot;\n             - name: ...\n</code></pre>\n<p>My <code>orders.sql</code> table looks like this:</p>\n<pre><code>{{config(materialized='table', file_format='parquet', location_root='path/to/storage2')}}\nselect name, age from {{ source('datalake', 'customers') }}\n</code></pre>\n<p>I'm also using the <code>dbt-external-tables</code> package. Also note that when I run <code>dbt debug</code> everything is fine and I can connect to my database (which happens to be Databricks).</p>\n<p>I tried running <code>dbt run-operation stage_external_sources</code> which returns <code>Error: staging external sources is not implemented for the default adapter</code>. When I run <code>dbt run</code>, I get <code>Error: UnresolvedRelation datalake.customers</code>.</p>\n<p>Or perhaps I could make use of the hive metastore instead somehow? Any tips on how I could fix this would be highly appreciated!</p>\n"}, {"tags": ["sql", "google-bigquery", "dbt"], "owner": {"account_id": 6284827, "reputation": 123, "user_id": 5335584, "user_type": "registered", "profile_image": "https://i.sstatic.net/SzUwN.jpg?s=256", "display_name": "Edgar", "link": "https://stackoverflow.com/users/5335584/edgar"}, "is_answered": true, "view_count": 3822, "accepted_answer_id": 66319143, "answer_count": 3, "score": 7, "last_activity_date": 1669202278, "creation_date": 1613735882, "question_id": 66277165, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66277165/how-to-choose-the-latest-partition-of-a-bigquery-table-in-dbt-without-scanning-t", "title": "How to choose the latest partition of a bigquery table in DBT without scanning the whole table?", "body": "<p>I'm trying to select the latest partition from a BigQuery table without scanning the whole table in a DBT model in order to save query costs.</p>\n<p>DBT doesnt allow using semicolons in a data model so using the <code>DECLARE</code>+<code>SET</code> scripting statements doesn't work as suggested <a href=\"https://stackoverflow.com/questions/39733826/how-to-choose-the-latest-partition-in-bigquery-table\">here</a>.</p>\n<p>DBT has a sql_header macro which allows setting some variables in the header but that header doesn't accept references to a data model or at least the following code is not compiling:</p>\n<pre><code>{{ config(\n  sql_header=&quot;  DECLARE latest_partition_date DATE;\n  DECLARE latest_load_timestamp TIMESTAMP;\n  SET latest_partition_date = (SELECT MAX(_PARTITIONDATE) FROM {{ ref(&quot;model&quot;) }} );\n  SET latest_load_timestamp = (SELECT MAX(loaded_at) FROM {{ ref(&quot;model&quot;) }} WHERE _PARTITIONDATE = latest_partition_date);&quot;\n) }}\n\n-- set the main query\nSELECT * FROM {{ ref(&quot;model&quot;) }}\nWHERE \n-- Select the latest partition to reduce 'Bytes processed' for loading the query.\n_PARTITIONDATE = latest_partition_date\n-- Select the latest load within the latest partition to get only one duplicate of data.\nAND loaded_at = latest_load_timestamp\n</code></pre>\n<p>I need to solve this in standard SQL.</p>\n<p>Other methods that were suggested included setting <code>WHERE _PARTITIONDATE = CURRENT_DATE()</code> or using <code>DATE_SUB(CURRENT_DATE(), 3)</code> but those don't satisfy because data load breakages are unpredictable and only dynamically selecting the latest would work here. Is that possible?</p>\n"}, {"tags": ["sql-server", "dbt"], "owner": {"account_id": 19534633, "reputation": 191, "user_id": 14293665, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/71407596d8492a4bb1a630b1aaa3ce4b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Sweety", "link": "https://stackoverflow.com/users/14293665/sweety"}, "is_answered": true, "view_count": 8367, "answer_count": 2, "score": 7, "last_activity_date": 1645940345, "creation_date": 1602073024, "question_id": 64243858, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64243858/can-we-convert-delete-sql-statements-into-dbt", "title": "can we convert delete SQL statements into DBT?", "body": "<p>I am trying to build a DBT model from SQL which has delete statements based on where clause.</p>\n<p>Can any one please suggest me how to convert the below SQL delete statement into DBT model?</p>\n<p>'''\ndelete table_name where condition;</p>\n<p>'''</p>\n<p>Thanks</p>\n"}, {"tags": ["snowflake-cloud-data-platform", "dbt"], "owner": {"account_id": 5555123, "reputation": 4753, "user_id": 4406793, "user_type": "registered", "accept_rate": 78, "profile_image": "https://www.gravatar.com/avatar/c1962611febcc643123f5a3049d99f04?s=256&d=identicon&r=PG", "display_name": "Marco Roy", "link": "https://stackoverflow.com/users/4406793/marco-roy"}, "is_answered": true, "view_count": 3650, "answer_count": 1, "score": 7, "last_activity_date": 1664488682, "creation_date": 1634681669, "last_edit_date": 1634764411, "question_id": 69638187, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69638187/invalid-jwt-token-happens-randomly-when-using-snowflakedbt", "title": "Invalid JWT token happens randomly when using Snowflake+DBT", "body": "<p>We've been using Snowflake+DBT with <a href=\"https://docs.snowflake.com/en/user-guide/key-pair-auth.html\" rel=\"noreferrer\">key pair authentication</a> for a long time now, and we've never had any issues.</p>\n<p>Recently, we started getting random connection errors on some models:</p>\n<pre><code>250001 (08001): Failed to connect to DB: account.region.snowflakecomputing.com:443. JWT token is invalid.\n</code></pre>\n<p>Most of the models will work, but some of them might fail. This can happen to any model, and it's never the same one -- sometimes it's the very first one, sometimes it's the very last one, and sometimes it's a bunch of them. It happens in runs with lots of models, or in runs with a single model.</p>\n<p>It's very inconsistent, and there doesn't seems to be any kind of pattern to it. Sometimes it works, and sometimes it doesn't.</p>\n<p>We've also tried with 1, 4, or 8 threads, and it happens regardless.</p>\n<p>Obviously, there's nothing wrong with the credentials or configurations \u2014 otherwise, nothing would run at all. So I assume there must be something wrong with how DBT is handling the connection(s).</p>\n<p>Interestingly, the errors only happens locally (so far). We haven't seen it in DBT Cloud runs.</p>\n<p>DBT version is 0.20.2 in both cases. We tried with other versions (0.21.0, 0.20.0 &amp; 0.19.1), and the issue persists. I don't know why we're just encountering this, as we've used these other versions previously without any issues.</p>\n<p>It's similar to <a href=\"https://stackoverflow.com/questions/65811588/snowflake-jdbc-driver-throws-net-snowflake-client-jdbc-snowflakesqlexception-jw\">this question</a>, except in our case it doesn't happen consistently at all. We tried connecting &quot;without a region&quot; (using <a href=\"https://docs.snowflake.com/en/user-guide/organizations.html\" rel=\"noreferrer\">Snowflake Organizations</a>), but it doesn't make any difference:</p>\n<pre><code>250001 (08001): Failed to connect to DB: organization-account.snowflakecomputing.com:443. JWT token is invalid.\n</code></pre>\n<p>Is there anything we can do to resolve this?</p>\n<p><em><strong>EDIT:</strong></em> When the error happens, the model hangs for 60 seconds until the error appears.</p>\n<p><em><strong>EDIT 2:</strong></em> I think the error might have started happening when we started using the <a href=\"https://hub.docker.com/r/fishtownanalytics/dbt\" rel=\"noreferrer\">DBT-provided Docker images</a>. Not sure exactly what might be wrong with them, but we'll try going back to our own custom images and see if that works.</p>\n"}, {"tags": ["databricks", "dbt", "databricks-unity-catalog"], "owner": {"account_id": 26212897, "reputation": 145, "user_id": 19891453, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AItbvmmik0TBJOsok-O4iHA8IJhF4-GuEQj6WD9zv_sH=k-s256", "display_name": "Ashley Betts", "link": "https://stackoverflow.com/users/19891453/ashley-betts"}, "is_answered": false, "view_count": 2643, "answer_count": 2, "score": 7, "last_activity_date": 1666735200, "creation_date": 1663023904, "last_edit_date": 1663787566, "question_id": 73696068, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73696068/dbt-on-databricks-unity-catalog", "title": "DBT on Databricks Unity Catalog", "body": "<p>I've been considering turning on Databricks Unity Catalog in our primary (only) workspace, but I'm concerned about how this might impact our existing dbt loads with the new three-part object references.</p>\n<p>I see from the dbt-databricks <a href=\"https://pypi.org/project/dbt-databricks/\" rel=\"nofollow noreferrer\">release notes</a> that you need &gt;= 1.1.1 to get unity support. <br>The snippet with it only shows setting the <em>catalog</em> property in the profile. I was planning on having some of the sources in separate catalog's for the dbt generated objects.</p>\n<p>I might even choose to have the dbt generated objects in separate catalogues if this was available.<br>\nAs turning on Unity Catalog is a one way road in a workspace, I don't wish to wing it and see what happens.</p>\n<p>Has anyone used dbt with Unity Catalog and used numerous catalogs in the project?</p>\n<p>If so, are there any gotcha's and how do you specify the catalog for sources and specific models?</p>\n<p>Regards,</p>\n<p>Ashley</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 22034448, "reputation": 141, "user_id": 16301202, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/89b605c54762a4e7b6ec5fee728ca89a?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Mark", "link": "https://stackoverflow.com/users/16301202/mark"}, "is_answered": true, "view_count": 5649, "accepted_answer_id": 69713706, "answer_count": 1, "score": 6, "last_activity_date": 1635190134, "creation_date": 1635186728, "last_edit_date": 1635190134, "question_id": 69713087, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69713087/dbt-relationship-test-compilation-error-test-definition-dictionary-must-have-ex", "title": "dbt relationship test compilation error: test definition dictionary must have exactly one key", "body": "<p>I'm a new user of dbt, trying to write a relationship test:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>- name: PROTOCOL_ID\n  tests:\n    - relationships:\n    to: ref('Animal_Protocols')\n    field: id\n</code></pre>\n<p>I am getting this error:</p>\n<pre><code>Compilation Error\nInvalid test config given in models/Animal_Protocols/schema.yml:\ntest definition dictionary must have exactly one key, got [('relationships', None), ('to', &quot;ref('Animal_Protocols')&quot;), ('field', 'id')] instead (3 keys)\n@: UnparsedNodeUpdate(original_file_path='model...ne)\n</code></pre>\n<p>&quot;unique&quot; and &quot;not-null&quot; tests in the same file are working fine, but I have a similar error with &quot;accepted_values&quot;.</p>\n<p>I am using dbt cli version 0.21.0 with Snowflake on MacOS Big Sur 11.6.</p>\n"}, {"tags": ["postgresql", "dbt"], "owner": {"account_id": 1226044, "reputation": 1832, "user_id": 1191545, "user_type": "registered", "accept_rate": 73, "profile_image": "https://i.sstatic.net/XQaPc.jpg?s=256", "display_name": "Brylie Christopher Oxley", "link": "https://stackoverflow.com/users/1191545/brylie-christopher-oxley"}, "is_answered": true, "view_count": 10551, "accepted_answer_id": 69774042, "answer_count": 3, "score": 6, "last_activity_date": 1635536706, "creation_date": 1612521873, "question_id": 66061826, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66061826/add-index-to-dbt-model-column", "title": "Add index to dbt model column?", "body": "<p>We are considering using dbt to manage models in our PostgreSQL data warehouse. Since dbt models are SQL select statements, there doesn't seem to be an obvious, or documented, way to specify that a particular column should have an index.</p>\n<p>How can we specify column indexes on dbt models?</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 10601951, "reputation": 73, "user_id": 7808631, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-q4UBGwxebGc/AAAAAAAAAAI/AAAAAAAAECk/2KuU4MufEK0/photo.jpg?sz=256", "display_name": "Chetan Grandhe", "link": "https://stackoverflow.com/users/7808631/chetan-grandhe"}, "is_answered": true, "view_count": 8273, "answer_count": 3, "score": 6, "last_activity_date": 1668417420, "creation_date": 1647975890, "question_id": 71577610, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71577610/dbt-doesnt-throw-an-error-when-running-dbt-docs-generate-but-the-catalog-json-i", "title": "DBT doesn&#39;t throw an error when running dbt docs generate but the catalog.json is missing", "body": "<p>I'm new to dbt, I am successfully able to create my models and schemas and macros, but when I do dbt docs generate I get manifest.json, but not catalog.json and so <code>dbt docs serve</code> is failing. It throws the below error.</p>\n<p><a href=\"https://i.sstatic.net/WPPrB.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/WPPrB.png\" alt=\"enter image description here\" /></a></p>\n<p>I checked through my logs I don't find an error, I ran the generated sql from logs in snowflake and it works as well.</p>\n<p>Below is what I get when I run dbt docs generate</p>\n<pre><code>(ENV) C:\\Users\\grands1\\Desktop\\Projects\\Project_name&gt; dbt docs generate\n18:45:10  Running with dbt=1.0.4\n18:45:10  Found 9 models, 4 tests, 0 snapshots, 0 analyses, 180 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\n18:45:10\n18:45:15  Concurrency: 1 threads (target='dev')\n18:45:15\n18:45:16  Done.\n18:45:16  Building catalog\n(ENV) C:\\Users\\grands1\\Desktop\\Projects\\Project_name&gt; \n\n</code></pre>\n<p>Is there a way to understand where this issue is coming from? Is it possible to better debug dbt docs generate command?</p>\n<p>Thank you,\nSai.</p>\n"}, {"tags": ["compiler-errors", "yaml", "dbt"], "owner": {"account_id": 19477684, "reputation": 101, "user_id": 14248767, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/1893494615b4da67e197309052aa9967?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Andresbi", "link": "https://stackoverflow.com/users/14248767/andresbi"}, "is_answered": true, "view_count": 18087, "accepted_answer_id": 68447240, "answer_count": 1, "score": 6, "last_activity_date": 1626730190, "creation_date": 1626694697, "question_id": 68439855, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68439855/in-dbt-when-i-add-a-well-formatted-yml-file-to-my-project-i-stop-being-able-t", "title": "In dbt, when I add a well formatted .yml file to my project, I stop being able to run dbt or compile SQL until I delete the yaml file. Why?", "body": "<p>Thank you in advance for helping me in my journey! I am a dbt newb, doing the <a href=\"https://courses.getdbt.com/courses/take/fundamentals/texts/17813360-learning-objectives\" rel=\"noreferrer\">dbt fundamentals course</a>. I am following the directions exactly.</p>\n<p>Sequence of the issue:</p>\n<ol>\n<li><p>I created .sql files in my models folder and subfolders, compiled them, ran dbt, and they showed up in my Snowflake DW. No problem</p>\n</li>\n<li><p>I added a .yml file to one of the sub folders\n<a href=\"https://i.sstatic.net/QLfPl.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/QLfPl.png\" alt=\"enter image description here\" /></a></p>\n</li>\n<li><p>The issue happens when I click &quot;save&quot; to save the code to the .yml file - that the course provided <a href=\"https://courses.getdbt.com/courses/take/fundamentals/texts/17704336-practice\" rel=\"noreferrer\">here</a>. (See compilation error in screenshot)</p>\n</li>\n</ol>\n<p><a href=\"https://i.sstatic.net/MCVAF.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/MCVAF.png\" alt=\"enter image description here\" /></a></p>\n<ol start=\"4\">\n<li>When I click on Compilation Error, I get this (Refreshing the IDE does not help):</li>\n</ol>\n<p>Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.\n'version'</p>\n<p><a href=\"https://i.sstatic.net/Les2W.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Les2W.png\" alt=\"enter image description here\" /></a></p>\n<ol start=\"5\">\n<li><p>At this point, I am not able to run any of the models, even those in other subfolders. For example\n<a href=\"https://i.sstatic.net/5sxOS.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/5sxOS.png\" alt=\"enter image description here\" /></a></p>\n</li>\n<li><p>Also, those .sql files also have the same compilation error above.</p>\n</li>\n<li><p>When I delete the .yml file, everything goes to normal and all the errors disappear.</p>\n</li>\n<li><p>things I have tried:</p>\n</li>\n</ol>\n<ul>\n<li>Deleting all the contents of the yml file and hitting save --&gt; the compilation error goes away</li>\n<li>Changing indentation</li>\n<li>Only leaving a single test instead of two</li>\n<li>Creating the yml file in a different subfolder</li>\n<li>Signing out of dbt and back in</li>\n</ul>\n<p>Please help!\nthank you</p>\n"}, {"tags": ["snowflake-cloud-data-platform", "dbt"], "owner": {"account_id": 17236657, "reputation": 213, "user_id": 12479987, "user_type": "registered", "profile_image": "https://i.sstatic.net/VEJoa.jpg?s=256", "display_name": "Mincho", "link": "https://stackoverflow.com/users/12479987/mincho"}, "is_answered": true, "view_count": 13659, "accepted_answer_id": 62996073, "answer_count": 1, "score": 6, "last_activity_date": 1601374231, "creation_date": 1595236781, "last_edit_date": 1601374231, "question_id": 62992304, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62992304/querying-with-dbt-from-external-source", "title": "Querying with dbt from external source", "body": "<p>I have the following issue:</p>\n<ul>\n<li>I have an AWS S3 pipeline that on a daily basis a single json.gz files is spit.</li>\n<li>I wish to take that file with dbt and put it into snowflake (no snowpipe use atm)</li>\n</ul>\n<p>I have managed to do this by creating a storage integration and I have manually created with my role (used for running dbt) a schema and assing usage on that schema. So far so good.</p>\n<p>Then I read about this:</p>\n<p><a href=\"https://github.com/fishtown-analytics/dbt-external-tables\" rel=\"nofollow noreferrer\">https://github.com/fishtown-analytics/dbt-external-tables</a></p>\n<p>Problem is that this is the only way this runs properly, I had to alter my dbt profiles.yml, set the default schema to be S3_MIXPANEL with default database RAW_DEV, run a different target and role on that with --target 'ingest_dev' parameter.</p>\n<p>I keep thinking that there should be a more sophisticated solution, where I can create schema's and query metadata and use something like {{ source() }} so I can point my documentation somehow that this is an external source. This dbt-external-tables is not really well explained for my case here I think?</p>\n<p>Please can anyone help me and share how to create schemas and query from external stages properly without changing default schema macro &amp;  dbtprofiles.yml each time?</p>\n<p>I have succeeded to run the following code:</p>\n<pre><code>{{\n  config(\n    materialized ='incremental',\n    schema = generate_schema_name('S3_MIXPANEL')\n  )\n}}\n \n  SELECT\n    metadata$filename as file_name,\n    to_date(SUBSTR(metadata$filename,16,10),'yyyy/mm/dd') as event_date,\n    $1 as payload,\n    CONVERT_TIMEZONE('Europe/London',TO_TIMESTAMP_tz($1:properties:mp_processing_time_ms::int / 1000)) as  event_timestamp_converted,\n    CONVERT_TIMEZONE('Europe/London', current_timestamp) as ingested_at\n\n from\n\n    @my_s3_stage\n\n    \n{% if is_incremental() %}\n    -- this filter will only be applied on an incremental run\n    WHERE event_date&gt;(\n    SELECT\n        max(event_date)\n    FROM\n        {{ this }}\n    )\n{% endif %}\n\n{{ row_limit() }} \n</code></pre>\n<p>EDIT 22-06-20:</p>\n<p>I have added the src_mixpanel.yml file in my models and ran the dbt command, however I had to also specify the data_types, so I added them too, then I apparently had to add the &quot;macro&quot; in my macros too (btw maybe a stupid question but I don't really know how to install your package, so I manually added all macros from yours into mine).</p>\n<p>Now when I run this code:</p>\n<pre><code>dbt run-operation stage_external_sources\n</code></pre>\n<p>with</p>\n<pre><code>version: 2\n\nsources:\n\n  - name: s3_mixpanel\n    database: RAW_DEV\n    tables:\n      - name: events\n        external:\n          location: '@my_s3_stage'\n          auto_refresh: false # depends on your S3 setup\n          partitions:\n            - name: event_date\n              expression: to_date(SUBSTR(metadata$filename,16,10),'yyyy/mm/dd')\n              data_type: date\n            - name: file_name\n              expression: metadata$filename\n              data_type: string\n          columns:\n            - name: properties\n              data_type: variant\n</code></pre>\n<p>I get an error:</p>\n<blockquote>\n<p>Encountered an error while running operation: Compilation Error in macro stage_external_sources (macros/stage_external_sources.sql)<br />\n'dict object' has no attribute 'sources'</p>\n</blockquote>\n"}, {"tags": ["snapshot", "dbt"], "owner": {"account_id": 25554789, "reputation": 141, "user_id": 19417549, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJzc9iXylA8X89UJr6s4yZwmm70XGNtgPMdmnf5h=k-s256", "display_name": "Cade Justad-Sandberg", "link": "https://stackoverflow.com/users/19417549/cade-justad-sandberg"}, "is_answered": true, "view_count": 3232, "accepted_answer_id": 72817843, "answer_count": 1, "score": 6, "last_activity_date": 1656600537, "creation_date": 1656210664, "last_edit_date": 1656210709, "question_id": 72758509, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72758509/encountered-unknown-tag-snapshot-in-dbt-snapshot", "title": "Encountered unknown tag &#39;snapshot&#39; in DBT Snapshot", "body": "<p>I want to run a DBT Snapshot and am following a near-identical template to the one outlined in the <a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/snapshots#snapshot-configurations\" rel=\"noreferrer\">documentation</a>. However, I get the error when I run <code>dbt snpashot</code></p>\n<pre><code>Compilation Error in model test_snapshot (.../project_folder/snapshots/test_snapshot.sql)    \nEncountered unknown tag 'snapshot'.\n        line 1\n          {% snapshot test_snapshot %}\n</code></pre>\n<p>Below is the code I am attempting to compile.</p>\n<pre><code>{% snapshot test_snapshot %}\n    {{\n        config(\n            strategy='check',\n            unique_key='id',\n            target_schema='snapshots',\n            check_cols= 'all'\n        )\n    }}\n\nselect\n        *\nfrom {{ ref('modle_in_sample_folder') }}\n\n{% endsnapshot %}\n</code></pre>\n<p>The order of the snapshot folder and ref file is .../project_folder/snapshots/test_snapshot.sql and .../project_folder/intermediate/model_in_sample_folder.sql</p>\n"}, {"tags": ["sql", "dbt"], "owner": {"account_id": 5310584, "reputation": 1554, "user_id": 4237080, "user_type": "registered", "accept_rate": 75, "profile_image": "https://graph.facebook.com/1210377595/picture?type=large", "display_name": "brienna", "link": "https://stackoverflow.com/users/4237080/brienna"}, "is_answered": true, "view_count": 16590, "accepted_answer_id": 72736650, "answer_count": 1, "score": 6, "last_activity_date": 1665422361, "creation_date": 1655990277, "question_id": 72730868, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72730868/get-column-names-and-types-using-star-macro-in-dbt", "title": "Get column names AND types using star macro in dbt", "body": "<p>Using the star macro, is there a way to also get the column data type (boolean, numerical, etc), in addition to the column name?</p>\n<p>For example, this query uses the star macro to collect the column names from a reference table, saves it as an array variable <code>column_names</code>, and then I loop over this array and apply the max function to all the columns.</p>\n<pre><code>{% set column_names = star(\n    from=ref_table,\n    except=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n    as_list=True)\n%}\n\nselect \n    date_trunc('week', day) as week,\n    name,\n\n    {%- for col in column_names %}  \n    max({{ col|lower }}) as {{ col | lower }}{%- if not loop.last %},{{ '\\n  ' }}{% endif %}\n    {%- endfor %}\n\nfrom {{ ref('my_table_name') }}    \ngroup by 1, 2\n</code></pre>\n<p>I would like to conditionally apply the max function to only boolean columns.</p>\n<p>This might look something like</p>\n<pre><code>{%- for col in column_names %}  \n    {% if is_boolean(col) %}  \n    max({{ col|lower }}) as {{ col | lower }}{%- if not loop.last %},{{ '\\n  ' }}{% endif %}\n    {% endif %}\n{%- endfor %}\n</code></pre>\n<p>but the problem is that the star macro passes the column names as a string, so it's not carrying any metadata with it.</p>\n<p>How might I get the column data type here?</p>\n<p>Data warehouse: Snowflake</p>\n"}, {"tags": ["python", "sql", "jinja2", "dbt"], "owner": {"account_id": 19745346, "reputation": 83, "user_id": 14457920, "user_type": "registered", "profile_image": "https://i.sstatic.net/sc5dd.jpg?s=256", "display_name": "Gus", "link": "https://stackoverflow.com/users/14457920/gus"}, "is_answered": true, "view_count": 21415, "answer_count": 2, "score": 6, "last_activity_date": 1623737250, "creation_date": 1623688239, "question_id": 67973996, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67973996/how-do-i-loop-through-nested-structs-using-jinja-in-dbt", "title": "How do I loop through nested structs using Jinja in DBT?", "body": "<p>I'm trying to build a model in DBT that flattens out a struct with name <code>properties</code> that contains about a hundred structs inside it (e.g. <code>property1</code>, <code>property2</code>, etc.), each with 5 different columns of which I want to extract one called <code>value</code>. I could type <code>properties.propertyX.value</code> 100 times, but I figured I could try to find a way to loop through each struct within <code>properties</code> and obtain <code>propertyX.value</code> within a <code>SELECT</code> statement by using Jinja, but I guess I'm either unfamiliar with the syntax or its limitations because I don't know what to do. I've tried:</p>\n<pre><code>WITH t as (\n    SELECT\n        properties\n    FROM\n        src\n)\nSELECT\n    {% for property in properties %}\n    {{property}}.value\n    {% endfor %}\n    {%- if not loop.last %},{% endif -%}\nFROM\n    t\n</code></pre>\n<p>but I realized I have to set <code>properties</code> as a variable and I don't really know how to do that in a way that it references the individual properties in the <code>properties</code> struct. Anyway, I'm quite lost and if someone could help I would be so grateful.</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 26212897, "reputation": 145, "user_id": 19891453, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AItbvmmik0TBJOsok-O4iHA8IJhF4-GuEQj6WD9zv_sH=k-s256", "display_name": "Ashley Betts", "link": "https://stackoverflow.com/users/19891453/ashley-betts"}, "is_answered": true, "view_count": 5983, "accepted_answer_id": 73589026, "answer_count": 2, "score": 6, "last_activity_date": 1711343203, "creation_date": 1661992822, "question_id": 73563223, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73563223/dbt-conditionally-set-schema-config", "title": "DBT: conditionally set schema config", "body": "<p>I'm trying to determine how I can conditionally set schema config attributes. I've attempted this by a macro in both dbt_project.yml and also in schema.yml but both of these methods fail with:</p>\n<pre><code>00:23:19  Encountered an error:\nCompilation Error\n  Could not render {{get_location_root('lndv')}}: 'get_location_root' is undefined\n</code></pre>\n<p>The outcome I would like to achieve is conditionally setting <em>location_root</em> for Spark for various schemas. I want different locations for each environment. I thought the macro path was the best fit as this follows a pattern but it obviously doesn't work in <em>dbt_project.yml</em> or property files. I was using <em>target.name</em> to determine environment. It's in the same directory as other macros that are successfully rendering in models so the path is set correctly. I don't really want to resort to placing this config in each model if I can avoid it.</p>\n<p>Does anyone have any thoughts on how I can solve this? Either getting the macro to work in <em>dbt_project.yml</em> / <em>schema.yml</em> or by some other method?</p>\n<p>Regards,</p>\n<p>Ashley</p>\n"}, {"tags": ["snowflake-cloud-data-platform", "dbt"], "owner": {"account_id": 15780469, "reputation": 422, "user_id": 11387016, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-HtaIyJwBGZo/AAAAAAAAAAI/AAAAAAAAGFA/OGcg1-mtDWo/photo.jpg?sz=256", "display_name": "ShayHa", "link": "https://stackoverflow.com/users/11387016/shayha"}, "is_answered": true, "view_count": 9059, "accepted_answer_id": 70414249, "answer_count": 1, "score": 6, "last_activity_date": 1640010000, "creation_date": 1639831570, "last_edit_date": 1639832557, "question_id": 70403550, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70403550/automatically-add-new-column-to-incremental-or-other-type", "title": "Automatically add new column to incremental (or other type)", "body": "<p>I need some wisdom for a new  DBT use case that I am trying to solve. I am pretty new to DBT and not sure what is the most efficient DBT way. We are using snowflake as our DWH.</p>\n<h3>Problem</h3>\n<p>We have a lot of incremental models that are managed with DBT. Lately, we had the need to add a new column to all models. What would be the most efficient DBT way to do it? Should we override the incremental macro script? (<a href=\"https://github.com/dbt-labs/dbt-snowflake/blob/main/dbt/include/snowflake/macros/materializations/incremental.sql\" rel=\"noreferrer\">I found this for snowflake</a>.) I assume that the last resort will be to add the new column manually to each model.</p>\n"}, {"tags": ["readability", "dbt", "schema.yml"], "owner": {"account_id": 9431343, "reputation": 93, "user_id": 7014797, "user_type": "registered", "profile_image": "https://graph.facebook.com/10154746549250967/picture?type=large", "display_name": "Kelly Danielle", "link": "https://stackoverflow.com/users/7014797/kelly-danielle"}, "is_answered": true, "view_count": 2623, "accepted_answer_id": 69561724, "answer_count": 1, "score": 6, "last_activity_date": 1673848954, "creation_date": 1634150262, "question_id": 69560670, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69560670/dbt-breaking-up-schema-yml-file", "title": "DBT breaking up schema.yml file", "body": "<p>We have a large schema.yml file in our DBT folders. It is not the cleanest or easiest to find what we need in it. I am curious if anyone knows of a way to split up this file. I am not trying to overcomplicate things and separate the dbt project into multiple or anything like that but rather just work on cleaning up the schema.yml file for readability etc. Thanks!</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 6677719, "reputation": 824, "user_id": 5151861, "user_type": "registered", "profile_image": "https://i.sstatic.net/DNhNg.jpg?s=256", "display_name": "iamtodor", "link": "https://stackoverflow.com/users/5151861/iamtodor"}, "is_answered": true, "view_count": 2534, "accepted_answer_id": 75094977, "answer_count": 3, "score": 6, "last_activity_date": 1688754456, "creation_date": 1659687908, "question_id": 73246787, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73246787/dbt-check-what-packages-are-installed", "title": "dbt: check what packages are installed", "body": "<p>Is there a way to check what packages are installed? I would expect something like <code>dbt list packages</code>?</p>\n<p>The context is:</p>\n<ul>\n<li>Until I run <code>dbt deps</code> the content of <code>packages.yml</code> gives me nothing. And there are some situations when the models could be triggered without running <code>dbt deps</code></li>\n<li>I would like to check the packages in runtime</li>\n</ul>\n<p>I searched over google and <code>dbt --help</code> but I didn't find anything.</p>\n"}, {"tags": ["python", "apache-spark", "dbt"], "owner": {"account_id": 20534012, "reputation": 181, "user_id": 15070559, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/7d9de67ac30e6e0d6dc6520f6588c58b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "sanchit08", "link": "https://stackoverflow.com/users/15070559/sanchit08"}, "is_answered": true, "view_count": 2502, "answer_count": 1, "score": 6, "last_activity_date": 1677197279, "creation_date": 1677152946, "last_edit_date": 1677153938, "question_id": 75544431, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75544431/how-to-override-global-dbt-macros-in-my-dbt-package-that-will-be-used-by-other-p", "title": "How to override global DBT macros in my dbt package that will be used by other projects", "body": "<p>I have a DBT package named <code>dbt_helpers</code>, where i intend to override some of dbt's in built global macros. In this example i intend to override the macro <code>dbt_spark_validate_get_file_format</code>, which is present in the dbt spark adapter <a href=\"https://github.com/dbt-labs/dbt-spark/blob/main/dbt/include/spark/macros/adapters.sql\" rel=\"noreferrer\">here</a>.</p>\n<p>I have referred the dbt docs specified <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/dispatch\" rel=\"noreferrer\">here</a> to implement my use case. Here is how i have implemented the macro in my package under package's <code>macros</code> folder.</p>\n<pre><code>{% macro dbt_spark_validate_get_file_format(raw_file_format) -%}\n    {{ return(adapter.dispatch('dbt_spark_validate_get_file_format','dbt_helpers')(raw_file_format)) }}\n{%- endmacro %}\n\n\n{% macro default__dbt_spark_validate_get_file_format(raw_file_format) %}\n    {% do log('overriding global macro', info=true) %}\n    {#  Custom implementation here  #}\n    \n{% endmacro %}\n</code></pre>\n<p>I have used the macro namespace <code>dbt_helpers</code> same as my package name. I have specified this in my main DBT project as a package in the <code>packages.yml</code> and I am able to see the macros defined in the <code>dbt_packages</code> directory after running the command <code>dbt deps</code>. In my main dbt project's <code>dbt_project.yml</code> I have included the project level dispatch config to take the macro from my package as shown, as directed in <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/dispatch#overriding-global-macros\" rel=\"noreferrer\">this</a> section of the dbt docs.</p>\n<pre><code>dispatch:\n  - macro_namespace: dbt\n    search_order: ['dbt_helpers','dbt']\n</code></pre>\n<p>However when I run my dbt model the macro defined in my package is not being called, rather the inbuilt global macro is still being called. I am able to override the macro by placing it directly inside my projects macros folder, but i need to override the macro from my <code>dbt_helpers</code> package. How can i manage to do this?</p>\n"}, {"tags": ["sql", "database", "list", "jinja2", "dbt"], "owner": {"account_id": 14897420, "reputation": 381, "user_id": 10757281, "user_type": "registered", "profile_image": "https://graph.facebook.com/10215776085348685/picture?type=large", "display_name": "Reid Williams", "link": "https://stackoverflow.com/users/10757281/reid-williams"}, "is_answered": true, "view_count": 20065, "answer_count": 1, "score": 5, "last_activity_date": 1667238833, "creation_date": 1597798493, "last_edit_date": 1597981192, "question_id": 63478564, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63478564/dbt-macro-to-iterate-over-item-in-list-within-a-sql-call", "title": "dbt macro to iterate over item in list within a sql call?", "body": "<p>First off, I am a dbt backer! I love this tool and the versatility of it.</p>\n<p>When reading some of the <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/adapter#drop_relation\" rel=\"noreferrer\">docs</a> I noticed that I might be able to do some meta work on my schemas every time I call a macro.</p>\n<p>One of those would be to clean up schemas.</p>\n<p>(<em>This has been edited as per discussion within the dbt slack</em>)</p>\n<ol>\n<li><p><code>dbt run-operation freeze</code> that would introspect all of the tables that would be written with dbt run but with an autogenerated hash (might just be timestamp). It would output those tables in the schema of my choice and would log the \u201chash\u201d to console.</p>\n</li>\n<li><p><code>dbt run-operation unfreeze --args '{hash: my_hash}'</code> that would then proceed to find the tables written with that hash prefix and clean them out of the schema.</p>\n</li>\n</ol>\n"}, {"tags": ["dbt"], "owner": {"account_id": 13032007, "reputation": 2260, "user_id": 9418115, "user_type": "registered", "profile_image": "https://i.sstatic.net/6YyIP.png?s=256", "display_name": "Adrien Arcuri", "link": "https://stackoverflow.com/users/9418115/adrien-arcuri"}, "is_answered": true, "view_count": 25253, "answer_count": 3, "score": 5, "last_activity_date": 1725483055, "creation_date": 1638965052, "question_id": 70274718, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70274718/how-to-declare-and-init-variable-in-a-dbt-model-in-sql-file-with-big-query-ad", "title": "How to declare and init variable in a dbt model in `.sql` file with big query adaptor?", "body": "<p>I would like to declare and init a variable in a dbt model <code>customer.sql</code> file.\nI used the keyword <code>DECLARE</code> to declare a variable like the <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/scripting\" rel=\"noreferrer\">BigQuery documentation</a> suggests but I got\na <code>Syntax error</code> on <code>DECLARE</code> keyword.</p>\n<p>Code:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>DECLARE myDate VARCHAR DEFAULT '2021-01-01';\n\nwith order_bis as (\n\n    select\n        order_id\n\n    from\n        order\n    where\n        customer_date &gt; myDate\n\n)\n\nselect * from order_bis\n</code></pre>\n<p>Error:\n<code>Syntax error: Expected &quot;(&quot; or keyword SELECT or keyword WITH but got keyword DECLARE ...</code></p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 19631558, "reputation": 51, "user_id": 14369764, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6a4672a71d863e44815a4ff0f2a91371?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "jayoub", "link": "https://stackoverflow.com/users/14369764/jayoub"}, "is_answered": true, "view_count": 9916, "answer_count": 1, "score": 5, "last_activity_date": 1643736388, "creation_date": 1601494081, "question_id": 64144483, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64144483/could-not-find-adapter-type-bigquery", "title": "Could not find adapter type bigquery", "body": "<p>Having trouble running dbt today... encountered this error message and cannot debug the issue. I did not have this issue yesterday and have not changed anything since.</p>\n<p>Installed dbt with Homebrew</p>\n<pre><code>Running with dbt=0.18.0\ndbt version: 0.18.0\n...\nConfiguration:\n  profiles.yml file [ERROR invalid]\n  dbt_project.yml file [OK found and valid]\nProfile loading failed for the following reason:\nRuntime Error\n  Credentials in profile &quot;dandelion-bq&quot;, target &quot;dev&quot; invalid: Runtime Error\n    Could not find adapter type bigquery!\n</code></pre>\n"}, {"tags": ["aws-lambda", "dbt"], "owner": {"account_id": 11806467, "reputation": 135, "user_id": 8638818, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bb796b69c31e5c45c02d2e122ec78980?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "mcanizo", "link": "https://stackoverflow.com/users/8638818/mcanizo"}, "is_answered": true, "view_count": 4554, "answer_count": 1, "score": 5, "last_activity_date": 1653414382, "creation_date": 1645720171, "question_id": 71255224, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71255224/how-to-run-dbt-in-aws-lambda", "title": "How to run DBT in AWS Lambda?", "body": "<p>I have currently dockerized my DBT solution and I launch it in AWS Fargate (triggered from Airflow). However, Fargate requires about 1 minute to start running (image pull + resource provisioning + etc.), which is great for long running executions (hours), but not for short ones (1-5 minutes).</p>\n<p>I'm trying to run my docker container in AWS Lambda instead of in AWS Fargate for short executions, but I encountered several problems during this migration.</p>\n<p>The one I cannot fix is related to the bellow message, at the time of running the <code>dbt deps --profiles-dir . &amp;&amp; dbt run -t my_target --profiles-dir . --select my_model</code></p>\n<pre><code>Running with dbt=0.21.0\nEncountered an error:\n[Errno 38] Function not implemented\n</code></pre>\n<p>It says there is no function implemented but I cannot see anywhere which is that function. As it appears at the time of installing dbt packages (redshift and dbt_utils), I tried to download them and include them in the docker image (set local paths in <em>packages.yml</em>), but nothing changed. Moreover, DBT writes no logs at this phase (I set the <em>log-path</em> to <em>/tmp</em> in the <em>dbt_project.yml</em> so that it can have write permissions within the Lambda), so I'm blind.</p>\n<p>Digging into this problem, I've found that this can be related to multiprocessing issues within AWS Lamba (my docker image contains python scripts), as stated in <a href=\"https://github.com/dbt-labs/dbt-core/issues/2992\" rel=\"noreferrer\">https://github.com/dbt-labs/dbt-core/issues/2992</a>. I run DBT from python using the <code>subprocess</code> library.</p>\n<p>Since it may be a multiprocessing issue, I have also tried to set <code>&quot;threads&quot;: 1</code> in <em>profiles.yml</em> but it did not solve the problem.</p>\n<p>Does anyone succeeded in deploying DBT in AWS Lambda?</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 1012277, "reputation": 20925, "user_id": 1024586, "user_type": "registered", "accept_rate": 87, "profile_image": "https://www.gravatar.com/avatar/3238fd4bbeafdce226ff3406e859f2d6?s=256&d=identicon&r=PG", "display_name": "Doug Fir", "link": "https://stackoverflow.com/users/1024586/doug-fir"}, "is_answered": true, "view_count": 8048, "accepted_answer_id": 75607081, "answer_count": 1, "score": 5, "last_activity_date": 1677691698, "creation_date": 1677653544, "question_id": 75600390, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75600390/exclude-multiple-models-from-run", "title": "Exclude multiple models from run", "body": "<p>When running dbt with descendants, I would like to exclude two models. I can exclude one model like so:</p>\n<pre><code>dbt run ga4_update_set+ --exclude nz_daily_cohorts\n</code></pre>\n<p>The above works as expected.</p>\n<p>I tried the following to exclude multiple models.</p>\n<pre><code>dbt run ga4_update_set+ --exclude nz_daily_cohorts,growth_scorecard\n</code></pre>\n<p>In this case neither nz_daily_cohorts nor growth_scorecard were excluded.</p>\n<p>Then tried:</p>\n<pre><code>dbt run ga4_update_set+ --exclude nz_daily_cohorts --exclude growth_scorecard\n</code></pre>\n<p>Again, in this case neither nz_daily_cohorts nor growth_scorecard were excluded.</p>\n<p>How can I run <code>dbt run ga4_update_set+</code> but also exclude both <code>nz_daily_cohorts</code> and <code>growth_scorecard</code>?</p>\n"}, {"tags": ["python", "snowflake-cloud-data-platform", "dbt"], "owner": {"account_id": 1740883, "reputation": 1053, "user_id": 1592334, "user_type": "registered", "accept_rate": 62, "profile_image": "https://www.gravatar.com/avatar/172b0cf44146a9046f6814cea0351806?s=256&d=identicon&r=PG", "display_name": "Lee", "link": "https://stackoverflow.com/users/1592334/lee"}, "is_answered": true, "view_count": 4888, "accepted_answer_id": 74032477, "answer_count": 1, "score": 5, "last_activity_date": 1674205079, "creation_date": 1665506357, "question_id": 74031424, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74031424/how-to-implement-python-udf-in-dbt", "title": "How to implement Python UDF in dbt", "body": "<p>Please I need some help with applying python UDF to run on my dbt models.\nI successfully created a python function in snowflake (DWH) and ran it against a table. This seems to work as expected, but implementing this on dbt seems to be a struggle. Some advice/help/direction will make my day.</p>\n<p>here is my python UDF created on snowflake</p>\n<pre><code>create or replace function &quot;077&quot;.&quot;Unity&quot;.sha3_512(str varchar)\nreturns varchar\nlanguage python\nruntime_version = '3.8'\nhandler = 'hash'\nas\n\n$$\nimport hashlib\n \ndef hash(str):\n    # create a sha3 hash object\n    hash_sha3_512 = hashlib.new(&quot;sha3_512&quot;, str.encode())\n\n    return hash_sha3_512.hexdigest()\n$$\n;\n</code></pre>\n<p>The objective is the create the python function in dbt and apply it to the model below</p>\n<pre><code>{{ config(materialized = 'view') }}\n\nWITH SEC AS(\n    SELECT \n         A.&quot;AccountID&quot; AS AccountID,\n         A.&quot;AccountName&quot; AS AccountName , \n         A.&quot;Password&quot; AS Passwords,\n apply function here (A.&quot;Password&quot;) As SHash\n    FROM {{ ref('Green', 'Account') }} A\n   )\n\n----------------VIEW RECORD------------------------------ \n\nSELECT * \nFROM SEC\n</code></pre>\n<p>is there a way to do this please. Thank you</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 21888305, "reputation": 51, "user_id": 16175677, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxJ8-UiLro2Iv9i8rdYoaC3wwi3M-IJUtYU6sK_=k-s256", "display_name": "Ilhomjon Oripov", "link": "https://stackoverflow.com/users/16175677/ilhomjon-oripov"}, "is_answered": true, "view_count": 22673, "answer_count": 4, "score": 5, "last_activity_date": 1672129109, "creation_date": 1623241733, "last_edit_date": 1645669358, "question_id": 67904146, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67904146/dbt-depends-on-a-source-not-found", "title": "dbt depends on a source not found", "body": "<p>Could you please help me with this issue?</p>\n<pre><code>Encountered an error:\nCompilation Error in model metrics_model (models\\example\\metrics_model.sql)\n  Model 'model.test_project.metrics_model' (models\\example\\metrics_model.sql) depends on a source named 'automate.metrics' which was not found\n</code></pre>\n<p>I am having this monotonous error, which I have not been able to solve.</p>\n<p>Many thanks beforehand!</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 15787495, "reputation": 51, "user_id": 11391802, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-psszHVuJp5Q/AAAAAAAAAAI/AAAAAAAAEM0/HdR5myImRvM/photo.jpg?sz=256", "display_name": "Manoj Kumar", "link": "https://stackoverflow.com/users/11391802/manoj-kumar"}, "is_answered": true, "view_count": 6312, "answer_count": 2, "score": 5, "last_activity_date": 1670844839, "creation_date": 1598904523, "last_edit_date": 1599226591, "question_id": 63677530, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63677530/dbt-cannot-create-two-resources-with-identical-database-representations", "title": "dbt cannot create two resources with identical database representations", "body": "<p>I have a situation here as below:</p>\n<p>There are two models in my dbt project</p>\n<ol>\n<li>model-A</li>\n</ol>\n<pre><code>{{ config(\n    materialized='ephemeral',\n    alias='A_0001',\n    schema=var('xxx_yyy_dataset')\n) }}\n</code></pre>\n<ol start=\"2\">\n<li>model-B</li>\n</ol>\n<pre><code>{{ config(\n    materialized='ephemeral',\n    alias='B_0002',\n    schema=var('xxx_yyy_dataset')\n) }}\n</code></pre>\n<p>And these are getting materialized as incremental in same schema as <code>xxx_yyy_dataset.Table_DDD</code></p>\n<pre><code>{{ config(\n    materialized='incremental',\n    alias='Table_DDD',\n    schema=var('xxx_yyy_dataset')\n) }}\nSELECT * FROM {{ref('A_0001')}}\nUNION ALL\nSELECT * FROM {{ref('B_0002')}}\n</code></pre>\n<p>This is working fine and it is ingesting records into target table.</p>\n<p>Now I have introduced another model - model-C ind different package\nmodel-C</p>\n<pre><code>{{ config(\n    materialized='incremental',\n    alias='Table_DDD',\n    schema=var('xxx_yyy_dataset')\n) }}\n</code></pre>\n<p>This gives me the following error:</p>\n<pre><code>$ dbt compile --profiles-dir=profile --target ide\nRunning with dbt=0.16.0\nEncountered an error:\nCompilation Error\n  dbt found two resources with the database representation &quot;xxx_yyy_dataset.Table_DDD&quot;.\n  dbt cannot create two resources with identical database representations. To fix this,\n  change the &quot;schema&quot; or &quot;alias&quot; configuration of one of these resources:\n  - model.eplus_rnc_dbt_project.conrol_outcome_joined (models/controls/payment/fa-join/conrol_outcome_joined.sql)\n  - model.eplus_rnc_dbt_project.dq_control_outcome_joined (models/controls/dq/dq-join/dq_control_outcome_joined.sql)\n</code></pre>\n<p>I have configured macro for custom macro as below :</p>\n<pre><code>{% macro generate_schema_name(custom_schema_name, node) -%}\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n        {{ default_schema }}\n    {%- else -%}\n        {{ custom_schema_name }}\n    {%- endif -%}\n{%- endmacro %}\n\n\n{% macro generate_alias_name(custom_alias_name=none, node=none) -%}\n    {%- if custom_alias_name is none -%}\n        {{ node.name }}\n    {%- else -%}\n        {{ custom_alias_name | trim }}\n    enter code here\n    {%- endif -%}\n{%- endmacro %}\n</code></pre>\n"}, {"tags": ["yaml", "dbt"], "owner": {"account_id": 3873970, "reputation": 21576, "user_id": 4281353, "user_type": "registered", "accept_rate": 69, "profile_image": "https://www.gravatar.com/avatar/2c965e9fbb680e2527285a75b1712497?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "mon", "link": "https://stackoverflow.com/users/4281353/mon"}, "is_answered": true, "view_count": 1468, "accepted_answer_id": 73876208, "answer_count": 2, "score": 5, "last_activity_date": 1666822595, "creation_date": 1664338788, "last_edit_date": 1664358686, "question_id": 73876165, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73876165/dbt-what-is-in-yaml-files", "title": "dbt - what is +/- in YAML files?", "body": "<p>In DBT YAML file such as dbt_project.yml, what is <code>+</code> or <code>-</code> sign of the element?</p>\n<pre><code>models:\n  # Be sure to namespace your model configs to your project name\n  dbt_labs:\n\n    # This configures models found in models/events/\n    events:\n      +enabled: true            # &lt;--- What is the meaning of +?\n      +materialized: view       # &lt;--- What is the meaning of +?\n\n      # This configures models found in models/events/base\n      # These models will be ephemeral, as the config above is overridden\n      base:\n        +materialized: ephemeral       # &lt;--- What is the meaning of +?\n</code></pre>\n"}, {"tags": ["snowflake-cloud-data-platform", "workflow", "dbt"], "owner": {"account_id": 18054890, "reputation": 51, "user_id": 13123614, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6510e38de5a659d76c3a318cc54f80fc?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Mirae Kim", "link": "https://stackoverflow.com/users/13123614/mirae-kim"}, "is_answered": true, "view_count": 2724, "answer_count": 2, "score": 5, "last_activity_date": 1653294409, "creation_date": 1643315064, "question_id": 70885262, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70885262/how-to-set-up-a-dev-to-prod-workflow-on-snowflake-and-dbt", "title": "How to set up a dev to prod workflow on snowflake (and dbt)?", "body": "<p>We are currently implementing snowflake and dbt and want to split snowflake databases between dev and prod, so that we have a database to test on before releasing new data models. We are planning to use dbt to create all of our data models going forward. I have a couple questions on the logistics of the workflow:</p>\n<ol>\n<li><p>How do we keep dev and prod in sync? (Or should they be?) I know in snowflake theres a clone feature you can recreate metadata without copying data over. Should we clone our prod database to dev? On a daily basis? What about users that have materialized resources in dev -- they would lose that data.</p>\n</li>\n<li><p>Should we make it so that deployment to prod was part of the CICD process, and only a fully merged pull request (tested on snowflake dev) can be deployed to the snowflake prod? Would that present too much of a bottle neck?</p>\n</li>\n</ol>\n<p>Curious to understand how people have architected their workflows maintaining both a dev and prod snowflake environment.</p>\n"}, {"tags": ["yaml", "markdown", "github-flavored-markdown", "dbt", "mkdocs"], "owner": {"account_id": 9319213, "reputation": 461, "user_id": 10713420, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/51001831aa59f592c10f668fcfb1c316?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "NAB0815", "link": "https://stackoverflow.com/users/10713420/nab0815"}, "is_answered": true, "view_count": 2490, "accepted_answer_id": 69399550, "answer_count": 1, "score": 5, "last_activity_date": 1633507139, "creation_date": 1632258585, "last_edit_date": 1633507139, "question_id": 69275594, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69275594/referencing-table-values-from-md-file-in-dbt-yaml-files", "title": "Referencing table values from md file in dbt yaml files", "body": "<p>I have a markdown file with a table in it and I have another dbt schema.yml file which is used to serve and generate docs. Traditionally I enter the table name and column name and description of the column name in schema.yml but now that I think of I'd want to reference the column names from the md document into the yaml file rather than manually entering.</p>\n<p>This is how my <code>doc.md</code> file looks like</p>\n<pre><code>{% docs column_description %}\n\n| COLUMN\\_NAME                   | DESCRIPTION                                                               |\n| ------------------------------ | ------------------------------------------------------------------------- |\n| cycle\\_id                      | Customer cycle\\_id(todays start sleep time to next days start sleep time) |\n| user\\_id                       | Customers user\\_id                                                        |\n| yes\\_alcohol                   | User consumed alcohol or not                                              |\n| blank\\_alcohol                 | User did not answer or user answered &quot;No&quot;                                 |\n                    \n\n\n{% enddocs %}\n</code></pre>\n<p>And, currently, this is how my schema.yml file looks</p>\n<pre><code>version: 2\n\nmodels:\n\n- name: journal_pivot_dev\n    description: One for for each journal entry of within one customer cycle\n    columns:\n      - name: cycle_id\n        description:  Customer cycleid\n        tests: &amp;not_null\n          - not_null:\n              severity: warn\n      - name: user_id\n        description: customer userid\n        tests:\n          - not_null:\n              severity: warn\n      - name: yes_alcohol\n        description: User consumed alcohol or not\n        tests: &amp;values_accepted\n          - accepted_values:\n              severity: warn\n              values: [0,1]\n      - name: blank_alcohol\n        description: User did not answer          \n        tests: *values_accepted \n</code></pre>\n<p>What I tried:</p>\n<pre><code>version: 2\n    \n    models:\n    \n    - name: journal_pivot_dev\n        description: One for for each journal entry of within one customer cycle\n        columns:\n          - name: cycle_id\n            description:  '{{ doc(column_description&quot;) }}'\n            tests: &amp;not_null\n              - not_null:\n                  severity: warn\n          - name: user_id\n            description: '{{ doc(column_description&quot;) }}'\n            tests:\n              - not_null:\n                  severity: warn\n          - name: yes_alcohol\n            description: '{{ doc(column_description&quot;) }}'\n            tests: &amp;values_accepted\n              - accepted_values:\n                  severity: warn\n                  values: [0,1]\n          - name: blank_alcohol\n            description: '{{ doc(column_description&quot;) }}'          \n            tests: *values_accepted \n</code></pre>\n<p>But when I do that the description is not rendering to just cycle_id but it is giving me the whole table in the md file.</p>\n<p><a href=\"https://i.sstatic.net/G8aQd.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/G8aQd.png\" alt=\"enter image description here\" /></a></p>\n<p>I am expecting something like this</p>\n<p><a href=\"https://i.sstatic.net/Z7v7V.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Z7v7V.png\" alt=\"enter image description here\" /></a></p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 16611383, "reputation": 51, "user_id": 12004301, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mCreIc1rUd8FdqL67UaVsfZXhLvlpqlTAwypGIS=k-s256", "display_name": "Ray Mar", "link": "https://stackoverflow.com/users/12004301/ray-mar"}, "is_answered": true, "view_count": 3409, "answer_count": 3, "score": 5, "last_activity_date": 1679425315, "creation_date": 1609890872, "last_edit_date": 1610017183, "question_id": 65588257, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65588257/macro-for-custom-schema-names-doesnt-apply-in-a-dbt-package", "title": "Macro for custom schema names doesn&#39;t apply in a dbt package", "body": "<p>I have an issue using custom schema names in a dbt package.</p>\n<p>I use the Macro provided in <a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/building-models/using-custom-schemas/#how-does-dbt-generate-a-models-schema-name\" rel=\"noreferrer\">dbt documentation</a>.</p>\n<pre><code>{% macro generate_schema_name(custom_schema_name, node) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n\n        {{ default_schema }}\n\n    {%- else -%}\n\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n</code></pre>\n<p>I put this macro in my dbt package here <a href=\"https://github.com/datafuel/covid-france_dbt/tree/0.0.1\" rel=\"noreferrer\">dbt package</a>.</p>\n<p>Finally I use this dbt package in another dbt project <a href=\"https://github.com/datafuel/DataPlatform_docker/tree/feature/dbt-package/dbt_repo/src\" rel=\"noreferrer\">dbt project</a>.</p>\n<p>Here is my dbt_project.yml in my dbt project :</p>\n<pre><code>name: 'covid_france'\nversion: '0.0.1'\nconfig-version: 2\n\nprofile: 'default'\n\nsource-paths: [&quot;models&quot;]\nanalysis-paths: [&quot;analysis&quot;]\ntest-paths: [&quot;tests&quot;]\ndata-paths: [&quot;data&quot;]\nmacro-paths: [&quot;macros&quot;]\nsnapshot-paths: [&quot;snapshots&quot;]\n\ntarget-path: &quot;target&quot;  \nclean-targets:         \n    - &quot;target&quot;\n    - &quot;dbt_modules&quot;\n</code></pre>\n<p>My dbt_project.yml in my dbt package :</p>\n<pre><code>name: 'covid_france'\nversion: '0.0.1'\nconfig-version: 2\n\nprofile: 'default'\n\nsource-paths: [&quot;models&quot;]\nanalysis-paths: [&quot;analysis&quot;]\ntest-paths: [&quot;tests&quot;]\ndata-paths: [&quot;data&quot;]\nmacro-paths: [&quot;macros&quot;]\nsnapshot-paths: [&quot;snapshots&quot;]\n\ntarget-path: &quot;target&quot;  \nclean-targets:         \n    - &quot;target&quot;\n    - &quot;dbt_modules&quot;\n\nmodels:\n    covid_france:\n        stg:\n            materialized: table\n            schema: stg\n        ods:\n            materialized: table\n            process-airbyte-outputs:\n                schema: ods\n            unions:\n                schema: ods\n        prs:\n            materialized: view\n      \n</code></pre>\n<p>When I try to run my dbt project, it imports the dbt package but doesn't apply the macro that is supposed to remove the main schema prefix (provided in profiles.yml) from custom schema names\nFor instance : the schema provided in my profiles.yml is &quot;prs&quot;. I have other custom schemas named ods and stg. But when dbt run, it create prs, prs_ods and prs_stg.</p>\n<p>The macro used to work fine when I use it directly in a dbt project (instead of putting it in a dbt package that I use in my dbt project)</p>\n<p>Thank you in advance !</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 23108720, "reputation": 173, "user_id": 17220802, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8be4ac303637cc833c46d2c32d443036?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Jb-99", "link": "https://stackoverflow.com/users/17220802/jb-99"}, "is_answered": true, "view_count": 2593, "accepted_answer_id": 71889468, "answer_count": 1, "score": 5, "last_activity_date": 1650061361, "creation_date": 1649727504, "last_edit_date": 1649729076, "question_id": 71836051, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71836051/declaring-multiple-warehouses-in-dbt", "title": "Declaring multiple warehouses in dbt", "body": "<p>I am pretty new to dbt , i want to use two kinds of warehouses in one project , currently i declared my clickhouse warehouse  which i am going to make tables for and i need to add another warehouse mindsDB becuase i want to reference some of the tables in it</p>\n<p>currently my prfofile.yml looks like this</p>\n<pre><code>dbt-project:\n  target: dev\n  outputs:\n    dev:\n      type: clickhouse\n      schema : clickhouse_l\n      host: 8.77.780.70\n      port: 6000\n      user: xxx\n      password: xxxx\n</code></pre>\n<p>i want to add the below warehouse too</p>\n<pre><code>type: mysql\nhost: mysql.mindsdb.com\nuser: mindsdb.user@example.com\npassword: xxx\nport: 3306\ndbname: mindsdb\nschema: exampl_xxx\nthreads: 1\n</code></pre>\n<p>is there a way to do it? thank you</p>\n"}, {"tags": ["jinja2", "snowflake-cloud-data-platform", "dbt"], "owner": {"account_id": 3690699, "reputation": 2713, "user_id": 3073340, "user_type": "registered", "accept_rate": 95, "profile_image": "https://www.gravatar.com/avatar/114cd763dfcabbe73f1aa28f95a7eb1b?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "sgdata", "link": "https://stackoverflow.com/users/3073340/sgdata"}, "is_answered": true, "view_count": 1646, "accepted_answer_id": 70371015, "answer_count": 1, "score": 5, "last_activity_date": 1639607068, "creation_date": 1633035594, "last_edit_date": 1633400909, "question_id": 69398772, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69398772/is-there-a-variable-list-of-all-valid-databases-schemas-combinations-in-dbt", "title": "Is there a variable list of all *valid* databases &amp; schemas combinations in dbt jinja?", "body": "<p>Doing a variation on <a href=\"https://discourse.getdbt.com/t/setting-up-snowflake-the-exact-grant-statements-we-run/439\" rel=\"nofollow noreferrer\">this example</a> for a macro (<code>grant_select_on_schemas.sql</code>) to set grants on a snowflake instance after dbt runs. My issue is that I've inherited a non-standard dbt build configuration which includes some statically defined <em>non-target</em> model locations.</p>\n<p>Examples:</p>\n<pre><code>snowflake-instance\n    |\n    |&gt; raw_db\n        |&gt; elt_schema_1\n        |&gt; elt_schema_2\n        |&gt; elt_schema_3\n    |&gt; utils_db\n        |&gt; calendar_schema_1\n    |&gt; staging_db\n        |&gt; elt_staging_1\n        |&gt; elt_staging_2\n        |&gt; elt_staging_3\n    |&gt; analytics_db\n        |&gt; core_models\n        |&gt; mart_1\n        |&gt; mart_2\n</code></pre>\n<p><code>profiles.yml</code></p>\n<pre><code>  target: prod\n  outputs:\n    prod:\n      type: snowflake\n      account: my-account.region-1\n      role: my-role\n\n      # User/password auth\n      user: &lt;user&gt;\n      password: &lt;pass&gt;\n\n      database: raw_db\n      warehouse: my-warehouse\n      schema: PUBLIC\n      threads: 2\n      client_session_keep_alive: False\n      query_tag: my-dbt-local\n</code></pre>\n<p><code>dbt-project.yml</code></p>\n<pre><code>models:\n    my-pro:\n        +materialized: table   \n        utils:\n            +database: UTILS\n            +materialized: table\n            calendar:\n                +schema: calendar_schema_1\n        staging:\n            +database: staging_db\n            +materialized: view\n            elt_staging_1:\n                +schema: elt_staging_1\n            elt_staging_2:\n                +schema: elt_staging_2\n            elt_staging_3:\n                +schema: elt_staging_3\n\n</code></pre>\n<p><code>grant_select_on_schemas.sql</code></p>\n<pre><code>-- macros/grants/grant_select_on_schemas.sql\n\n{% macro grant_select_on_schemas(schemas, role) %}\n  {% for schema in schemas %}\n    {% for role in roles %}\n      grant usage on schema {{ schema }} to role {{ role }};\n      grant select on all tables in schema {{ schema }} to role {{ role }};\n      grant select on all views in schema {{ schema }} to role {{ role }};\n      grant select on future tables in schema {{ schema }} to role {{ role }};\n      grant select on future views in schema {{ schema }} to role {{ role }};\n    {% endfor %}\n  {% endfor %}\n{% endmacro %}\n</code></pre>\n<p>Currently, I'm running into the issue with this macro that the macro is attempting to run on against all schemas on my profile's <code>{{ target.database }}</code> (which is currently set to <code>staging_db</code>) and as a consequence is erroring when attempting things like:</p>\n<pre><code>&gt; Database Error\n&gt;   002003 (02000): SQL compilation error:\n&gt;   Schema 'staging_db.core_models' does not exist or not authorized.\n</code></pre>\n<p>What am I missing?</p>\n"}, {"tags": ["stored-procedures", "dbt", "snowflake-cloud-data-platform"], "owner": {"account_id": 4920634, "reputation": 63, "user_id": 3963284, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/19585b71b4d21f462c717b224fa83209?s=256&d=identicon&r=PG", "display_name": "Bardia", "link": "https://stackoverflow.com/users/3963284/bardia"}, "is_answered": true, "view_count": 3538, "accepted_answer_id": 69444774, "answer_count": 2, "score": 5, "last_activity_date": 1636637668, "creation_date": 1631611894, "question_id": 69175318, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69175318/snowflake-stored-procedure-fails-from-dbt", "title": "Snowflake stored procedure fails from dbt", "body": "<p>I have a problem to execute a stored procedure in Snowflake by dbt:</p>\n<p>The description of my procedure is like: MyStoredProcedure(ARRAY, VARCHAR, VARCHAR)</p>\n<p>So, when I want to run it, I use <strong>array_construct</strong> function to create the first argument, for example:\n<code>call MyStoredProcedure(array_construct(array_construct('str_1', 'str_2')), 'schema_name', 'table_name');</code></p>\n<p>This works when I run it in Snowflake. However, when I run this from dbt it fails with this error:</p>\n<blockquote>\n<p>Modifying a transaction that has started at a different scope is not allowed.</p>\n</blockquote>\n<p>Which I am sure that it is something related to invoking array_construct in this call.</p>\n<p>I should mention that to run this from dbt I have defined a macro like this:</p>\n<pre><code>{% macro MyStoredProcedure() %}\n    {% set query -%}\n        CALL MyStoredProcedure(\n           array_construct(array_construct('str_1', 'str_2')),\n           'schema_name',\n           'table_name');\n    {%- endset %}\n\n    {% do run_query(query) %}\n{% endmacro %}\n</code></pre>\n<p>And run it of course like: <code>dbt run-operation MyStoredProcedure</code></p>\n<p>I appreciate any tip or idea to help me with this problem.</p>\n<p>Thanks</p>\n"}, {"tags": ["databricks", "dbt"], "owner": {"account_id": 6729535, "reputation": 1904, "user_id": 5682512, "user_type": "registered", "accept_rate": 73, "profile_image": "https://www.gravatar.com/avatar/0b543a3897886a2bee848c0a72598144?s=256&d=identicon&r=PG", "display_name": "k88", "link": "https://stackoverflow.com/users/5682512/k88"}, "is_answered": true, "view_count": 3013, "accepted_answer_id": 71206919, "answer_count": 2, "score": 5, "last_activity_date": 1648047594, "creation_date": 1644247109, "last_edit_date": 1645173887, "question_id": 71020949, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71020949/cant-connect-dbt-to-databricks", "title": "Can&#39;t connect dbt to Databricks", "body": "<p>I am trying to connect to a Spark cluster on Databricks and I am following this tutorial: <a href=\"https://docs.databricks.com/dev-tools/dbt.html\" rel=\"nofollow noreferrer\">https://docs.databricks.com/dev-tools/dbt.html</a>. And I have the <code>dbt-databricks</code> connector installed (<a href=\"https://github.com/databricks/dbt-databricks\" rel=\"nofollow noreferrer\">https://github.com/databricks/dbt-databricks</a>). However, no matter how I configure it, I keep getting &quot;Database error, failed to connect&quot; when I run <code>dbt test</code> / <code>dbt debug</code>.</p>\n<p>This is my <code>profiles.yaml</code>:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>databricks_cluster:\n  outputs:\n    dev:\n      connect_retries: 5\n      connect_timeout: 60\n      host: &lt;my_server_hostname&gt;\n      http_path: &lt;my_http_path&gt;\n      schema: default\n      token: &lt;my_token&gt;\n      type: databricks\n  target: dev\n</code></pre>\n<p>This is my <code>dbt_project.yml</code>:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code># Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'dbt_dem'\nversion: '1.0.0'\nconfig-version: 2\n\n# This setting configures which &quot;profile&quot; dbt uses for this project.\nprofile: 'databricks_cluster'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the &quot;models/&quot; directory. You probably won't need to change these!\nmodel-paths: [&quot;models&quot;]\nanalysis-paths: [&quot;analyses&quot;]\ntest-paths: [&quot;tests&quot;]\nseed-paths: [&quot;seeds&quot;]\nmacro-paths: [&quot;macros&quot;]\nsnapshot-paths: [&quot;snapshots&quot;]\n\ntarget-path: &quot;target&quot;  # directory which will store compiled SQL files\nclean-targets:         # directories to be removed by `dbt clean`\n  - &quot;target&quot;\n  - &quot;dbt_packages&quot;\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\n# In this example config, we tell dbt to build all models in the example/ directory\n# as tables. These settings can be overridden in the individual model files\n# using the `{{ config(...) }}` macro.\nmodels:\n  dbt_dem:\n    # Config indicated by + and applies to all files under models/example/\n    example:\n      +materialized: view\n</code></pre>\n<p>I have also tried using the <code>spark</code> connector, but I still get the same error using that. Any ideas as to why I can't connect to the Databricks cluster?</p>\n<p>These are the logs corresponding to the error:</p>\n<pre><code>============================== 2022-02-18 08:43:22.123066 | 4b91f9d3-28ad-4f5a-93db-f431b6d9af14 ==============================\n08:43:22.123066 [info ] [MainThread]: Running with dbt=1.0.1\n08:43:22.123841 [debug] [MainThread]: running dbt with arguments Namespace(cls=&lt;class 'dbt.task.debug.DebugTask'&gt;, config_dir=False, debug=None, defer=None, event_buffer_size=None, fail_fast=None, log_cache_events=False, log_format=None, partial_parse=None, printer_width=None, profile=None, profiles_dir='/Users/keremaslan/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, send_anonymous_usage_stats=None, single_threaded=False, state=None, static_parser=None, target=None, use_colors=None, use_experimental_parser=None, vars='{}', version_check=None, warn_error=None, which='debug', write_json=None)\n08:43:22.124057 [debug] [MainThread]: Tracking: tracking\n08:43:22.143750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb751ef42e0&gt;, &lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb751ef4eb0&gt;, &lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb751eca730&gt;]}\n08:43:22.236001 [debug] [MainThread]: Executing &quot;git --help&quot;\n08:43:22.264682 [debug] [MainThread]: STDOUT: &quot;b&quot;usage: git [--version] [--help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;]\\n           [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path]\\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\\n           [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;]\\n           &lt;command&gt; [&lt;args&gt;]\\n\\nThese are common Git commands used in various situations:\\n\\nstart a working area (see also: git help tutorial)\\n   clone             Clone a repository into a new directory\\n   init              Create an empty Git repository or reinitialize an existing one\\n\\nwork on the current change (see also: git help everyday)\\n   add               Add file contents to the index\\n   mv                Move or rename a file, a directory, or a symlink\\n   restore           Restore working tree files\\n   rm                Remove files from the working tree and from the index\\n   sparse-checkout   Initialize and modify the sparse-checkout\\n\\nexamine the history and state (see also: git help revisions)\\n   bisect            Use binary search to find the commit that introduced a bug\\n   diff              Show changes between commits, commit and working tree, etc\\n   grep              Print lines matching a pattern\\n   log               Show commit logs\\n   show              Show various types of objects\\n   status            Show the working tree status\\n\\ngrow, mark and tweak your common history\\n   branch            List, create, or delete branches\\n   commit            Record changes to the repository\\n   merge             Join two or more development histories together\\n   rebase            Reapply commits on top of another base tip\\n   reset             Reset current HEAD to the specified state\\n   switch            Switch branches\\n   tag               Create, list, delete or verify a tag object signed with GPG\\n\\ncollaborate (see also: git help workflows)\\n   fetch             Download objects and refs from another repository\\n   pull              Fetch from and integrate with another repository or a local branch\\n   push              Update remote refs along with associated objects\\n\\n'git help -a' and 'git help -g' list available subcommands and some\\nconcept guides. See 'git help &lt;command&gt;' or 'git help &lt;concept&gt;'\\nto read about a specific subcommand or concept.\\nSee 'git help git' for an overview of the system.\\n&quot;&quot;\n08:43:22.265387 [debug] [MainThread]: STDERR: &quot;b''&quot;\n08:43:22.272505 [debug] [MainThread]: Acquiring new databricks connection &quot;debug&quot;\n08:43:22.273434 [debug] [MainThread]: Using databricks connection &quot;debug&quot;\n08:43:22.273833 [debug] [MainThread]: On debug: select 1 as id\n08:43:22.274044 [debug] [MainThread]: Opening a new connection, currently in state init\n08:43:22.888586 [debug] [MainThread]: Databricks adapter: Error while running:\nselect 1 as id\n08:43:22.889031 [debug] [MainThread]: Databricks adapter: Database Error\n  failed to connect\n08:43:22.889905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb751f7eaf0&gt;, &lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb752113040&gt;, &lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb7521130a0&gt;]}\n08:43:24.130154 [debug] [MainThread]: Connection 'debug' was properly closed.\n</code></pre>\n"}, {"tags": ["amazon-web-services", "yaml", "dbt"], "owner": {"account_id": 24346813, "reputation": 263, "user_id": 18283242, "user_type": "registered", "profile_image": "https://i.sstatic.net/wZcQP.jpg?s=256", "display_name": "Aliaksandra", "link": "https://stackoverflow.com/users/18283242/aliaksandra"}, "is_answered": false, "view_count": 1455, "answer_count": 0, "score": 5, "last_activity_date": 1670277293, "creation_date": 1654796084, "question_id": 72564447, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72564447/how-to-add-description-to-the-test-in-dbt", "title": "how to add description to the test in dbt", "body": "<p>How to add description to the generic test which is usually written in yml so that I can see it in the DESCRIPTION field in dbt docs</p>\n<p>in yml such tests are usually written like this but how to add a description to the test</p>\n<pre><code>        - name: desc\n         description: &quot;it is the description of the column but not the test&quot;\n          tests:\n          - not_null:\n</code></pre>\n<p><a href=\"https://i.sstatic.net/xpYko.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/xpYko.png\" alt=\"enter image description here\" /></a></p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 6802882, "reputation": 395, "user_id": 5235665, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b1b829c8707d74031ecd37125d6d2f16?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "hotmeatballsoup", "link": "https://stackoverflow.com/users/5235665/hotmeatballsoup"}, "is_answered": true, "view_count": 608, "accepted_answer_id": 76519301, "answer_count": 2, "score": 5, "last_activity_date": 1696591488, "creation_date": 1686942733, "last_edit_date": 1686943068, "question_id": 76492997, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/76492997/automatically-generated-python-dbt-test-seems-to-be-hardcoded-to-fail", "title": "Automatically-generated Python dbt test seems to be hardcoded to fail", "body": "<p>I have a Python DBT project that defines the following data model (via YAML):</p>\n<pre><code>version: 2\nmodels:\n- name: company\n  description: ''\n  columns:\n  - name: ID\n    description: Unique identifier for the company entity. It is the company name\n    tests:\n      - unique\n      - not_null\n      - dbt_expectations.expect_column_values_to_be_of_type:\n          column_type: VARCHAR\n  - name: REMOTE_ID\n    description: Company name\n    tests:\n      - dbt_expectations.expect_column_values_to_be_of_type:\n          column_type: VARCHAR\n  - name: CREATED_AT\n    description: ''\n    tests:\n      - dbt_expectations.expect_column_values_to_be_of_type:\n          column_type: TIMESTAMP_TZ\n      - not_null\n  - name: UPDATED_AT\n    description: ''\n    tests:\n      - dbt_expectations.expect_column_values_to_be_of_type:\n          column_type: TIMESTAMP_TZ\n</code></pre>\n<p>When I run <code>dbt compile</code>, then <code>dbt run</code> then <code>dbt test</code> I get a DBT test failure with the following console output:</p>\n<pre><code>17:59:30  Failure in test dbt_expectations_expect_column_values_to_be_of_type_company_CREATED_AT__TIMESTAMP_TZ (models/fizzbuzz/domain/company.yml)\n17:59:30    Got 1 result, configured to fail if != 0\n17:59:30  \n17:59:30    compiled Code at target/compiled/myapp/models/fizzbuzz/domain/company.yml/dbt_expectations_expect_column_837854ce21e79ee1abe42f69891c68e9.sql\n</code></pre>\n<p>When I go to that compiled SQL at <code>dbt_expectations_expect_column_837854ce21e79ee1abe42f69891c68e9.sql</code> this is what I see:</p>\n<pre><code>WITH relation_columns\n     AS (SELECT Cast('ID' AS VARCHAR)      AS relation_column,\n                Cast('VARCHAR' AS VARCHAR) AS relation_column_type\n         UNION ALL\n         SELECT Cast('REMOTE_ID' AS VARCHAR) AS relation_column,\n                Cast('VARCHAR' AS VARCHAR)   AS relation_column_type\n         UNION ALL\n         SELECT Cast('CREATED_AT' AS VARCHAR)    AS relation_column,\n                Cast('TIMESTAMP_NTZ' AS VARCHAR) AS relation_column_type\n         UNION ALL\n         SELECT Cast('UPDATED_AT' AS VARCHAR)    AS relation_column,\n                Cast('TIMESTAMP_NTZ' AS VARCHAR) AS relation_column_type),\n     test_data\n     AS (SELECT *\n         FROM   relation_columns\n         WHERE  relation_column = 'CREATED_AT'\n                AND relation_column_type NOT IN ( 'TIMESTAMP_TZ' ))\nSELECT *\nFROM   test_data \n</code></pre>\n<p>So my <em>understanding</em> is that when I run <code>dbt compile</code> it generates this test above from the compiled YAML file definition. And that when I subsequently run <code>dbt test</code>, it runs this SQL to determine whether the test defined in that YAML passes or fails.</p>\n<p>If that's correct, don't we have a bug in DBT here? Is it me or isn't this a false negative test that is hard-coded to always fail?!</p>\n<p>It's creating a temp table/CTE called <code>test_data</code> that is in no way, shape or form connecting to my <code>company</code> database table, but that has 2 columns (<code>relation_column</code> and <code>relation_column_type</code>). Then its giving this table a row where the <code>relation_column</code> value is &quot;CREATED_AT&quot; and its <code>relation_column_type</code> value is &quot;TIMESTAMP_NTZ&quot;. And finally its returning any records whose <code>relation_column</code> value is &quot;CREATED_AT&quot; but whose <code>relation_column_type</code> value is <strong>not</strong> &quot;TIMESTAMP_NTZ&quot;...so wouldn't this always fail?!</p>\n<p>Or am I missing something <strong>big</strong> about how DBT and its tests work? Thanks for any clarifications here.</p>\n"}, {"tags": ["api", "data-analysis", "transformation", "data-transform", "dbt"], "owner": {"account_id": 5036339, "reputation": 6375, "user_id": 4044988, "user_type": "registered", "accept_rate": 40, "profile_image": "https://lh3.googleusercontent.com/-HexM6y2dAAU/AAAAAAAAAAI/AAAAAAAAABI/0iGjyusWH8A/photo.jpg?sz=256", "display_name": "MegaBytes", "link": "https://stackoverflow.com/users/4044988/megabytes"}, "is_answered": true, "view_count": 7802, "answer_count": 1, "score": 4, "last_activity_date": 1651033364, "creation_date": 1613479209, "question_id": 66224513, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66224513/can-we-call-any-external-rest-api-inside-dbtdata-build-tool", "title": "Can we call any external REST API inside DBT(Data Build Tool)?", "body": "<p>I am working on some analytical work and we need to transform data from one source to another and we are using <a href=\"https://www.getdbt.com/\" rel=\"nofollow noreferrer\">DBT</a> for transformation purpose. one of the data available to use via only REST API. so my question is can we call external API inside dbt file and extract the fields from its response. Do we have something?</p>\n"}, {"tags": ["google-cloud-platform", "google-cloud-composer", "dbt"], "owner": {"account_id": 21808310, "reputation": 41, "user_id": 16106224, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gh-iwo8e_CxvPl-fXbF5Vr0bhK-O84uTLaLoZNB=k-s256", "display_name": "Michel van Dijck", "link": "https://stackoverflow.com/users/16106224/michel-van-dijck"}, "is_answered": true, "view_count": 3076, "answer_count": 2, "score": 4, "last_activity_date": 1679456027, "creation_date": 1622639996, "last_edit_date": 1622703563, "question_id": 67805961, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67805961/how-to-set-up-dbt-with-google-cloud-composer", "title": "How to set up dbt with Google Cloud Composer?", "body": "<p>I am trying to install dbt on Google Cloud Composer but run into dependency issues. I have followed the instructions from this article: <a href=\"https://blog.doit-intl.com/setup-dbt-with-cloud-composer-ab702454e27b\" rel=\"nofollow noreferrer\">https://blog.doit-intl.com/setup-dbt-with-cloud-composer-ab702454e27b</a> however at step 2: installing the packages (airflow-dbt &amp; dbt) in composer, it already fails.</p>\n<p>I find the following in the Cloud build logs:</p>\n<pre><code>ERROR: snowflake-connector-python 2.3.6 has requirement boto3&lt;1.16,&gt;=1.4.4, but you'll have boto3 1.17.85 which is incompatible.\nERROR: snowflake-connector-python 2.3.6 has requirement requests&lt;2.24.0, but you'll have requests 2.24.0 which is incompatible.\nERROR: networkx 2.5.1 has requirement decorator&lt;5,&gt;=4.3, but you'll have decorator 5.0.9 which is incompatible.\nERROR: hologram 0.0.13 has requirement jsonschema&lt;3.2,&gt;=3.0, but you'll have jsonschema 3.2.0 which is incompatible.\nERROR: dbt-core 0.19.1 has requirement idna&lt;2.10, but you'll have idna 2.10 which is incompatible.\nERROR: dbt-core 0.19.1 has requirement requests&lt;2.24.0,&gt;=2.18.0, but you'll have requests 2.24.0 which is incompatible.\nERROR: dbt-snowflake 0.19.1 has requirement cryptography&lt;4,&gt;=3.2, but you'll have cryptography 3.0 which is incompatible.\nERROR: dbt-bigquery 0.19.1 has requirement google-api-core&lt;1.24,&gt;=1.16.0, but you'll have google-api-core 1.28.0 which is incompatible.\nERROR: dbt-redshift 0.19.1 has requirement boto3&lt;1.16,&gt;=1.4.4, but you'll have boto3 1.17.85 which is incompatible.\n</code></pre>\n<p>My current environment configuration contains: composer-1.13.0-airflow-1.10.12</p>\n<p>Has anyone encountered the same problem and have you been able to solve it?\nI have also tried to install the specific versions of the requirements listed in the logs but this does not resolve the problem.</p>\n"}, {"tags": ["jinja2", "dbt"], "owner": {"account_id": 21860059, "reputation": 43, "user_id": 16150972, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9646241412eec3c002902f4fc364b349?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "joboro", "link": "https://stackoverflow.com/users/16150972/joboro"}, "is_answered": true, "view_count": 7087, "accepted_answer_id": 70789055, "answer_count": 2, "score": 4, "last_activity_date": 1697640905, "creation_date": 1636988248, "last_edit_date": 1636989951, "question_id": 69976423, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69976423/dbt-utils-surrogate-key-with-all-fields-except-two", "title": "dbt_utils.surrogate_key() with all fields except two", "body": "<p>I am currently playing with dbt and trying to build a PSA (Persistent Staging Area) and the snapshot functionality lends itself well to this in my eyes. However, I don't have timestamps in the sources, so I have to use the &quot;check&quot; strategy.</p>\n<p>For &quot;check_cols&quot; I would like to use a hash value, so I thought of dbt_utils.surrogate_key().</p>\n<p>But I would like to calculate the hash value over all columns except the two columns that are always the same.</p>\n<p>So my model looks like this:</p>\n<pre><code>{% snapshot Item_hist %}\n{{\n    config(\n      unique_key='item_id',\n      strategy='check',\n      check_cols=['diff_hash'],\n      target_database='PSA',\n      target_schema='sourceA',\n      alias= 'Item',\n      invalidate_hard_deletes=True\n    )\n}}\n\nselect {{ dbt_utils.surrogate_key(['Tenant','ItemNo']) }} as item_id,\n{{ dbt_utils.surrogate_key( dbt_utils.star(from=source('sourceA', 'item'), except=[&quot;fieldA&quot;, &quot;fieldB&quot;]) ) }} as diff_hash,\n* \nfrom {{ source('sourceA', 'item') }}\n{% endsnapshot %}\n</code></pre>\n<p>Unfortunately, the dbt_utils.surrogate_key() cannot handle the return value of dbt_utils.star().\nHow could I proceed here so that surrogate_key() can calculate a hash value from the return?</p>\n"}, {"tags": ["docker", "dbt"], "owner": {"account_id": 15622610, "reputation": 2266, "user_id": 11271048, "user_type": "registered", "profile_image": "https://i.sstatic.net/8thDR.jpg?s=256", "display_name": "alt-f4", "link": "https://stackoverflow.com/users/11271048/alt-f4"}, "is_answered": true, "view_count": 10392, "accepted_answer_id": 65489276, "answer_count": 6, "score": 4, "last_activity_date": 1695911189, "creation_date": 1609088641, "last_edit_date": 1609235400, "question_id": 65468231, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65468231/how-can-i-configure-dbt-dependencies-without-manually-running-dbt-deps", "title": "How can I configure DBT Dependencies without manually running dbt deps?", "body": "<p>I am new to DBT and currently trying to build a Docker container where I can directly run DBT commands within. I have a file where I export env variables (<code>envs.sh</code>) that looks like:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>export DB_HOST=&quot;secret&quot;\nexport DB_PWD=&quot;evenabiggersecret&quot;\n</code></pre>\n<p>My <code>packages.yml</code> looks like:</p>\n<pre><code>packages:\n  - package: fishtown-analytics/dbt_utils\n    version: 0.6.2\n</code></pre>\n<p>I structured my docker file like:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM fishtownanalytics/dbt:0.19.0b1\n# Define working directory\nWORKDIR /usr/app/profile/\nENV DBT_DIR /usr/app\nENV DBT_PROFILES_DIR /usr/app\n# Load ENV Vars\nCOPY ./dbt ${DBT_DIR}\n# Load env variables and install packages\nCOPY envs.sh envs.sh\nRUN . ./envs.sh \\\n &amp;&amp; dbt deps # Exporting envs to avoid profile not found errors when install deps\n</code></pre>\n<p>However, when I run <code>dbt run</code> inside the docker container I get the error:\n<code>'dbt_utils' is undefined</code>. When I manually run <code>dbt deps</code> it seems to fix the issue and <code>dbt run</code> succeeds. Am I missing something when I am originally installing the dependencies?</p>\n<p>Update:\nIn other words, running <code>dbt deps</code> when building the Docker image seems to have no effect. So I have to run it manually (when I do docker run for example) before I can start doing my workflows. This issue does not happen when I use a Python image (not the image from fishtown-analytics)</p>\n"}, {"tags": ["snowflake-cloud-data-platform", "dbt"], "owner": {"account_id": 21040750, "reputation": 65, "user_id": 15463238, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gi-bc7esDtde9mqLJb3noaFL2Hl4ASTaylzXMxI=k-s256", "display_name": "PolFrx", "link": "https://stackoverflow.com/users/15463238/polfrx"}, "is_answered": true, "view_count": 17838, "accepted_answer_id": 74572646, "answer_count": 3, "score": 4, "last_activity_date": 1700585307, "creation_date": 1669224263, "question_id": 74550798, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74550798/is-there-a-way-to-force-a-full-refresh-for-a-dbt-incremental-model-if-its-model", "title": "Is there a way to force a full refresh for a dbt incremental model if its model changed?", "body": "<p>I'll implement dbt for pipelines in Snowflake with incremental models to save query costs but I want to manage the changes of schemas that will be quite frequent. I will have one daily ETL job for each env running a <code>dbt run</code>.\nAlso, in qa and prod environments I'll not be able to run any cmd as I don't have access to these environments for security issues, only to dev.</p>\n<p>Is it possible to trigger a full refresh of a model if its schema changed?</p>\n<p>I saw that we can use the <code>on_schema_change</code> option with incremental models but this will just add (or drop) columns without populating them which is not exactly what I'm looking for as I'll not be able to run a force refresh manually in qa and prod.</p>\n<p>Thanks a lot</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 25367234, "reputation": 41, "user_id": 19181640, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gi6gjjbC-HNcD92lel5ggeJ8DqYJdKWvncwIser=k-s256", "display_name": "Jason McKenzie", "link": "https://stackoverflow.com/users/19181640/jason-mckenzie"}, "is_answered": true, "view_count": 3307, "answer_count": 3, "score": 4, "last_activity_date": 1721790992, "creation_date": 1653322526, "question_id": 72351740, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72351740/dbt-deps-the-process-cannot-access-the-file-because-it-is-being-used-by-anothe", "title": "dbt deps - The process cannot access the file because it is being used by another process: &#39;dbt_packages\\\\dbt-expectations-0.5.1\\\\integration_tests&#39;", "body": "<p>I last used dbt several months ago.  Returning to it, I had to update from version 0.21.1 to 1.1.0 to match my team's project.  I've run dbt clean, then dbt deps always returns this error: <a href=\"https://i.sstatic.net/WIWBE.png\" rel=\"nofollow noreferrer\">The process cannot access the file because it is being used by another process: 'dbt_packages\\dbt-expectations-0.5.1\\integration_tests'</a></p>\n<p>I see two packages after it errors:  dbt_expectations &amp; dbt-expectations-0.5.1 - I'm not sure what the latter package is about since it doesn't exist on my teammates machines.</p>\n<p>These are all my packages which match my teammates:</p>\n<ul>\n<li>package: calogica/dbt_date\nversion: 0.5.1</li>\n<li>package: dbt-labs/dbt_utils\nversion: 0.8.0</li>\n<li>package: dbt-labs/codegen\nversion: 0.5.0</li>\n<li>package: calogica/dbt_expectations\nversion: 0.5.1</li>\n</ul>\n<p>There's nothing else using the mentioned file before executing dbt deps &amp; I've restarted to ensure there wasn't anything hung up.  I don't see anything helpful in the logs:</p>\n<blockquote>\n<p>14:28:05.062468 [debug] [MainThread]: Sending event: {'category':\n'dbt', 'action': 'package', 'label':\n'5cc0ad0f-792a-49a6-8a66-f59ff8c3e642', 'property_': 'install',\n'context': [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson\nobject at 0x000002CDFB6D1EB0&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x000002CDFB6D14C0&gt;]} 14:28:05.063465 [info ] [MainThread]: Installing\ncalogica/dbt_expectations 14:28:06.961306 [debug] [MainThread]:\nSending event: {'category': 'dbt', 'action': 'invocation', 'label':\n'end', 'context':\n[&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x000002CDFB689550&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x000002CDFB6D14C0&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x000002CDFB702EE0&gt;]}</p>\n<p>============================== 2022-05-23 14:28:10.424049 | 2a7e240e-1707-4551-8966-58797e038d3a ==============================\n14:28:10.424049 [info ] [MainThread]: Running with dbt=1.1.0\n14:28:10.425046 [debug] [MainThread]: running dbt with arguments\n{'write_json': True, 'use_colors': True, 'printer_width': 80,\n'version_check': True, 'partial_parse': True, 'static_parser': True,\n'profiles_dir': 'C:\\Users\\jasonmckenzie\\.dbt',\n'send_anonymous_usage_stats': True, 'event_buffer_size': 100000,\n'quiet': False, 'no_print': False, 'resource_types': [], 'output':\n'selector', 'indirect_selection': 'eager', 'which': 'list'}\n14:28:10.425046 [debug] [MainThread]: Tracking: tracking\n14:28:10.435018 [debug] [MainThread]: Sending event: {'category':\n'dbt', 'action': 'invocation', 'label': 'start', 'context':\n[&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000254727F3D90&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000254727F32E0&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000254727F37C0&gt;]} 14:28:10.462945 [debug] [MainThread]: Sending\nevent: {'category': 'dbt', 'action': 'invocation', 'label': 'end',\n'context': [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson\nobject at 0x00000254727D1820&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000254727D1490&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000254727D19D0&gt;]}</p>\n<p>============================== 2022-05-23 14:28:10.825973 | 8d0ab879-6b59-4473-845a-cde9dd23352b ==============================\n14:28:10.825973 [info ] [MainThread]: Running with dbt=1.1.0\n14:28:10.826970 [debug] [MainThread]: running dbt with arguments\n{'write_json': True, 'use_colors': True, 'printer_width': 80,\n'version_check': True, 'partial_parse': True, 'static_parser': True,\n'profiles_dir': 'C:\\Users\\jasonmckenzie\\.dbt',\n'send_anonymous_usage_stats': True, 'event_buffer_size': 100000,\n'quiet': False, 'no_print': False, 'resource_types': [], 'output':\n'selector', 'indirect_selection': 'eager', 'which': 'list'}\n14:28:10.826970 [debug] [MainThread]: Tracking: tracking\n14:28:10.835946 [debug] [MainThread]: Sending event: {'category':\n'dbt', 'action': 'invocation', 'label': 'start', 'context':\n[&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000170D052F100&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000170D052FEB0&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000170D052FC40&gt;]} 14:28:10.892797 [debug] [MainThread]: Sending\nevent: {'category': 'dbt', 'action': 'invocation', 'label': 'end',\n'context': [&lt;snowplow_tracker.self_describing_json.SelfDescribingJson\nobject at 0x00000170D04EC640&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000170D04ECD90&gt;,\n&lt;snowplow_tracker.self_describing_json.SelfDescribingJson object at\n0x00000170D04EC4F0&gt;]}</p>\n</blockquote>\n"}, {"tags": ["dbt"], "owner": {"account_id": 23603389, "reputation": 81, "user_id": 17639491, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0dd4676de94d74fcc8eac050345fbeb1?s=256&d=identicon&r=PG", "display_name": "NavySeal2026", "link": "https://stackoverflow.com/users/17639491/navyseal2026"}, "is_answered": true, "view_count": 9783, "answer_count": 1, "score": 4, "last_activity_date": 1680516146, "creation_date": 1674146529, "question_id": 75175545, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75175545/setting-dbt-date-variable", "title": "Setting dbt date variable", "body": "<p>I am trying to set a date variable within a dbt model to be the date 7 days ago. The model will run against a Redshift database. I have done the following to set the variable, however I get the error DATE_ADD is not defined:</p>\n<p><code>{%- set start_date = TRUNC(DATE_ADD(day, -7, CURRENT_DATE)) -%}</code></p>\n<p>What is the correct way to set the variable.</p>\n"}, {"tags": ["python", "jinja2", "dbt"], "owner": {"account_id": 21393716, "reputation": 335, "user_id": 17488892, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GirYtT8uPj8AbR06sA04q4pruJIlZ9D_igoeq__Sg=k-s256", "display_name": "Aleksandra", "link": "https://stackoverflow.com/users/17488892/aleksandra"}, "is_answered": true, "view_count": 7388, "accepted_answer_id": 73364308, "answer_count": 1, "score": 4, "last_activity_date": 1662425911, "creation_date": 1660579833, "question_id": 73363627, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73363627/pass-a-macro-as-a-parameter-jinja-dbt", "title": "pass a macro as a parameter jinja dbt", "body": "<p>{{ today_date_milliseconds() }} - is my macro in the project. How to redirect this macro as a parameter, so it will be by default and I could in yml write another macro?</p>\n<pre><code>{% test valid_date(model, column_name, exclude_condition = '1=1') %}\n\n    SELECT {{ column_name }}\n    FROM {{ model }}\n    WHERE (CAST( {{ column_name }} AS BIGINT) &lt; {{ today_date_milliseconds() }}\n    AND {{ exclude_condition }}\n\n{% endtest %}\n</code></pre>\n<p>In yml it will look like</p>\n<pre><code>        - name: date_3\n          description: column for third date\n          tests:\n          - valid_date:\n                lower_bound: 'name of another macro'\n\n</code></pre>\n"}, {"tags": ["variables", "yaml", "jinja2", "dbt"], "owner": {"account_id": 15705200, "reputation": 115, "user_id": 11332127, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-w4LN0XNtDhk/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rd6aR90Kl4bTWjOLooRhd4hXJat7Q/mo/photo.jpg?sz=256", "display_name": "Mrinal", "link": "https://stackoverflow.com/users/11332127/mrinal"}, "is_answered": true, "view_count": 3235, "answer_count": 2, "score": 4, "last_activity_date": 1649194665, "creation_date": 1638272611, "last_edit_date": 1638290193, "question_id": 70168808, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70168808/nested-variables-in-dbt-project-yml-file-of-dbt", "title": "Nested variables in dbt_project.yml file of dbt", "body": "<p>Below is my code in dbt_project.yml file</p>\n<pre><code>vars:\n    # Variable \n    project1:\nvendor:\n    ABC\n    DEF\n\nABC:\n    model:\n        name: model123\n    case_types:\n        name: CASE1\n        name: CASE2\n        name: CASE3\n        name: CASE4\n</code></pre>\n<p>the way i am trying to access this in model is below. When the below code is run the values in src are ('model' &amp; 'case_types') respectively. How do i access values of these( 'model123, CASE1, CASE2....)</p>\n<pre><code>{% set vars1 =  var('ABC') %}\n{% for src in vars1 %}\n  {{log(src, True)}}\n{% endfor %}\n</code></pre>\n"}, {"tags": ["python", "etl", "dbt"], "owner": {"account_id": 17979288, "reputation": 351, "user_id": 13066054, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gh6MZsCRyDro27I5q923fET00GK6T2-b6e-VpUswA=k-s256", "display_name": "naga satish", "link": "https://stackoverflow.com/users/13066054/naga-satish"}, "is_answered": true, "view_count": 9722, "accepted_answer_id": 74855292, "answer_count": 2, "score": 4, "last_activity_date": 1671618670, "creation_date": 1671450645, "last_edit_date": 1671618670, "question_id": 74850128, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74850128/macros-are-not-recognised-in-dbt", "title": "macros are not recognised in dbt", "body": "<pre><code>{{ \n    config (\n        pre_hook = before_begin(&quot;{{audit_tbl_insert(1,'stg_news_sentiment_analysis_incr') }}&quot;),\n        post_hook = after_commit(&quot;{{audit_tbl_update(1,'stg_news_sentiment_analysis_incr','dbt_development','news_sentiment_analysis') }}&quot;)\n        )\n}}\n\nselect rd.news_id ,rd.title, rd.description, ns.sentiment from live_crawler_output_rss.rss_data rd \nleft join \nlive_crawler_output_rss.news_sentiment ns \non rd.news_id = ns.data_id limit 10000;\n</code></pre>\n<p>This is my model in DBT which is configured with pre and post hooks which referance a macro to insert and update the audit table.</p>\n<p>my macro</p>\n<pre><code>{ % macro audit_tbl_insert (model_id_no, model_name_txt) % }\n\n{% set run_id_value = var('run_id') %}\n\ninsert into {{audit_schema_name}}.{{audit_table_name}} (run_id, model_id, model_name, status, start_time, last_updated_at)\nvalues \n({{run_id_value}}::bigint,{{model_id_no}}::bigint,{{model_name_txt}},'STARTED',current_timestamp,current_timestamp)\n\n{% endmacro %}\n\n</code></pre>\n<p>this is the first time i'm using this macro and I see the following error.</p>\n<pre><code>Compilation Error in model stg_news_sentiment_analysis_incr \n(models/staging/stg_news_sentiment_analysis_incr.sql)\n'audit_tbl_insert' is undefined in macro run_hooks (macros/materializations/hooks.sql) \ncalled by macro materialization_table_default (macros/materializations/models/table/table.sql) called by model stg_news_sentiment_analysis_incr \n(models/staging/stg_news_sentiment_analysis_incr.sql). \nThis can happen when calling a macro that does not exist. \nCheck for typos and/or install package dependencies with &quot;dbt deps&quot;.\n</code></pre>\n"}, {"tags": ["dbt"], "owner": {"account_id": 24619873, "reputation": 41, "user_id": 18520400, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxnnTPQ1Hc9KVjq9ABj6KzUPnR8WfTkKvPJ02TZ=k-s256", "display_name": "Ofer Elrom", "link": "https://stackoverflow.com/users/18520400/ofer-elrom"}, "is_answered": true, "view_count": 2408, "answer_count": 1, "score": 4, "last_activity_date": 1689068956, "creation_date": 1647782136, "last_edit_date": 1663742108, "question_id": 71547080, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/71547080/dbt-recursionerror", "title": "DBT RecursionError", "body": "<p>I've installed <code>dbt</code> on my Windows PC.\nAfter creating the necessary configuration, I've executed <code>dbt debug</code> successfully.\nI tried executing <code>dbt run</code>. I have a local Postgres installed, and I'm getting the following error:</p>\n<p><code>RecursionError: maximum recursion depth exceeded while calling a Python object</code></p>\n<h5>Output of <code>dbt --version</code></h5>\n<pre><code>installed version: 1.0.4\n   latest version: 1.0.4\n\nUp to date!\n\nPlugins:\n  - postgres: 1.0.4 - Up to date!\n</code></pre>\n<p>Anything I can do?</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 19173924, "reputation": 113, "user_id": 14007029, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/820a1857421f0da2fe7a6e7826aea40f?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "joellabes", "link": "https://stackoverflow.com/users/14007029/joellabes"}, "is_answered": true, "view_count": 3063, "accepted_answer_id": 69655584, "answer_count": 1, "score": 4, "last_activity_date": 1634787504, "creation_date": 1634787141, "question_id": 69655537, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69655537/dbt-run-select-x-gives-error-could-not-find-selector-named-x-expected-one", "title": "`dbt run --select x` gives error `Could not find selector named x, expected one of []`", "body": "<p>Using dbt 0.20.x and below, the command <code>dbt run --select model_name</code> fails.</p>\n<p>It shows the error <code>Runtime Error. Could not find selector named model_name, expected one of [] Code: 10001</code>.</p>\n"}, {"tags": ["web", "model", "documentation", "dbt", "fishtown-analytics"], "owner": {"account_id": 4160423, "reputation": 643, "user_id": 14899625, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/477d0bd5a135d51df1a7c8842b3d7b4a?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "user961", "link": "https://stackoverflow.com/users/14899625/user961"}, "is_answered": true, "view_count": 1515, "accepted_answer_id": 69192862, "answer_count": 1, "score": 4, "last_activity_date": 1631707827, "creation_date": 1631701938, "question_id": 69191415, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69191415/dbt-docs-generate-override-the-default-overview-page-with-custom-content-in-th", "title": "DBT docs generate - Override the default overview page with custom content in the documentation website", "body": "<p>I am using the dbt docs generate for generating my project's documentation website. I want to override the default overview page with my own custom content in the website. Is it possible to do that?</p>\n"}, {"tags": ["python", "postgresql", "dbt"], "owner": {"account_id": 16130802, "reputation": 673, "user_id": 11644523, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/2a7d35b9fef8b4c817abf9e35b84f5e2?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Dametime", "link": "https://stackoverflow.com/users/11644523/dametime"}, "is_answered": true, "view_count": 778, "accepted_answer_id": 75022353, "answer_count": 1, "score": 4, "last_activity_date": 1672940426, "creation_date": 1672932325, "question_id": 75020740, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75020740/dbt-postgres-all-models-appending-schema-public-to-output", "title": "dbt postgres - all models appending schema public_ to output", "body": "<p>I am testing a local setup of dbt-postgres. I have a simple model, but for some reason, any table created is being placed in a schema with the prefix <code>public</code> appended to it.</p>\n<p>Desired output table:\n<code>public.test</code></p>\n<p>Current output table:\n<code>public_public.test</code></p>\n<p>As you can see, the public schema is being duplicated here. Using another schema in the model also creates a new schema with <code>public_</code> prefix.</p>\n<p>Simple model test.sql file:</p>\n<pre><code>{{ config(materialized='table', schema='public') }}\n\nselect a from table x\n</code></pre>\n<p>dbt_profile.yml</p>\n<pre><code>name: 'abc'\nversion: '0.1'\nconfig-version: 2\nprofile: 'abc'\nmodel-paths: [&quot;models&quot;]\nanalysis-paths: [&quot;analyses&quot;]\ntest-paths: [&quot;tests&quot;]\nseed-paths: [&quot;seeds&quot;]\nmacro-paths: [&quot;macros&quot;]\nsnapshot-paths: [&quot;snapshots&quot;]\n\ntarget-path: &quot;target&quot; \nclean-targets:       \n  - &quot;target&quot;\n  - &quot;dbt_packages&quot;\n  - &quot;dbt_modules&quot;\n  - &quot;logs&quot;\n\nmodels:\n  abc:\n      materialized: table\n</code></pre>\n<p>profiles.yml</p>\n<pre><code>abc:\n  outputs:\n    dev:\n      type: postgres\n      threads: 1\n      host: &quot;localhost&quot;\n      port: 5432\n      user: &quot;admin&quot;\n      pass: &quot;admin&quot;\n      dbname: database\n      schema: &quot;public&quot;\n  target: dev\n</code></pre>\n"}, {"tags": ["macos", "yaml", "jinja2", "dbt"], "owner": {"account_id": 26951930, "reputation": 45, "user_id": 20521017, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/ALm5wu1NHJHWRmp8G6RLxs2M7oD2se0XzcYnVT3DE-W4=k-s256", "display_name": "TheLoneGrape", "link": "https://stackoverflow.com/users/20521017/thelonegrape"}, "is_answered": true, "view_count": 8238, "accepted_answer_id": 74864260, "answer_count": 1, "score": 4, "last_activity_date": 1712136749, "creation_date": 1671500514, "last_edit_date": 1671500579, "question_id": 74857734, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74857734/with-dbt-how-do-i-use-a-jinja-macro-in-a-yaml-file", "title": "With dbt. How do I use a jinja macro in a yaml file", "body": "<p>I have a yaml file like this:</p>\n<pre><code>models:\n       - name: test_view\n        description: &quot;test&quot;\n        config:\n          meta:\n            database_tags:\n              ACCOUNT_OBJECTS.TAGS.ENV: DEV`\n\n\n</code></pre>\n<p>I am trying automatically change 'DEV' to PROD when it's in that environment. I have a macro that gets the variable from targets.name</p>\n<p>This is the jinja code:</p>\n<pre><code>{% macro test_macro(target) %}\n      {%- if  target.name == &quot;dev&quot; -%} DEV\n      {%- elif target.name == &quot;prod&quot;  -%} PROD\n      {%- else -%} invalid\n      {%- endif -%}\n    {% endmacro %}`\n</code></pre>\n<p>However, when I try to use the macro I get 'test_macro is undefined'</p>\n<p>eg. ACCOUNT_OBJECTS.TAGS.ENV: {{ test_macro(target)}}</p>\n<p>Is it that custom macros still cannot be used in yaml files?</p>\n"}, {"tags": ["sql", "macros", "jinja2", "dbt"], "owner": {"account_id": 6699569, "reputation": 157, "user_id": 5166548, "user_type": "registered", "profile_image": "https://i.sstatic.net/g5zz3.jpg?s=256", "display_name": "Marcos Rull&#225;n", "link": "https://stackoverflow.com/users/5166548/marcos-rull%c3%a1n"}, "is_answered": true, "view_count": 2736, "accepted_answer_id": 72115350, "answer_count": 1, "score": 4, "last_activity_date": 1724045874, "creation_date": 1651479716, "question_id": 72084073, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72084073/declare-var-in-other-macro-to-be-used-in-a-different-macro-for-dbt", "title": "Declare var in other macro to be used in a different macro for DBT", "body": "<p>The main idea is to have a constant variable and then have the possibility to use it in other parts of the code or macros for DBT.</p>\n<p>Example of a macro that contains constants:</p>\n<pre><code>{% macro constant_vars() -%}\n    \n{%\nset var_1 = {\n &quot;0&quot;: [&quot;0&quot;],\n &quot;1&quot;: [&quot;1&quot;, &quot;11&quot;, &quot;111&quot;]\n}\n%}\n\n{%\nset var_2 = {\n &quot;2&quot;: [&quot;2&quot;],\n &quot;3&quot;: [&quot;3&quot;]\n}\n%}\n{%- endmacro -%}\n</code></pre>\n<p>Macro that use a constant from the previous macro:</p>\n<pre><code>{% macro evaluate(\n    column_to_check\n) -%}\n\nCASE\n    {% for map_key in var_1 -%}\n    WHEN ({{column_to_check}} IN UNNEST( {{ var_1[map_key] }})) THEN\n        '{{ map_key }}'\n    {% endfor -%}\n    ELSE\n        &quot;-1&quot;\nEND\n{%- endmacro -%}\n</code></pre>\n<p>SQL sentence created for DBT:</p>\n<pre><code>SELECT \n[..]\n  evaluate(column1)\n[..]\nFROM\n table\n</code></pre>\n<p>DBT compiled query:</p>\n<pre><code>SELECT \n[..]\nCASE\n    WHEN (column1 IN UNNEST([&quot;0&quot;])) THEN\n        '0'\n    WHEN (column1 IN UNNEST([&quot;1&quot;, &quot;11&quot;, &quot;111&quot;])) THEN\n        '1'\n    \n    ELSE\n        &quot;-1&quot;\nEND\n[..]\nFROM\n table\n</code></pre>\n<p>Is it possible? Exist another way to do that?</p>\n<p>Thanks!</p>\n"}, {"tags": ["sql", "jinja2", "snowflake-cloud-data-platform", "dbt", "snowflake-connector"], "owner": {"account_id": 17775413, "reputation": 98, "user_id": 12907887, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6560ad22058b5da93dc8a2dc1e892f67?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Lewis Brogan", "link": "https://stackoverflow.com/users/12907887/lewis-brogan"}, "is_answered": true, "view_count": 1532, "accepted_answer_id": 70615757, "answer_count": 1, "score": 4, "last_activity_date": 1641521031, "creation_date": 1641492952, "question_id": 70611726, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70611726/using-data-build-tooldbt-and-snowflake-how-can-i-check-if-a-column-is-a-date", "title": "Using data build tool(dbt) and snowflake, how can I check if a column is a date field?", "body": "<p><strong>I'm a Junior so apologies if my explanation isn't that great.</strong></p>\n<p>I've created a macro on dbt to add a default row with defined values or default values based on data type.</p>\n<p>What I'm trying to achieve is to check if the column is a datatype <code>date</code> field, then it will return the default variable <code>{{ date_vi }}</code> which I've defined as <code>'1900-00-00'</code>, but I'm getting an error:</p>\n<p><code>dbt.adapters.snowflake.column.SnowflakeColumn object' has no attribute 'isdate</code> which tells me there is no <code>is_date()</code> which is confusing because <code>is_date()</code> works on snowflake normally.</p>\n<p>I have now noticed on the dbt docs:</p>\n<p><a href=\"https://docs.getdbt.com/reference/dbt-classes#column\" rel=\"nofollow noreferrer\">https://docs.getdbt.com/reference/dbt-classes#column</a></p>\n<p>and the source code on github for snowflake:</p>\n<p><a href=\"https://github.com/dbt-labs/dbt-snowflake/blob/main/dbt/adapters/snowflake/column.py\" rel=\"nofollow noreferrer\">https://github.com/dbt-labs/dbt-snowflake/blob/main/dbt/adapters/snowflake/column.py</a></p>\n<p>That is_date() isn't actually available with the snowflake adapter, the code I was trying to get working was: <code>{% elif col.is_date() %}{{ date_vl }}</code> so I'm wondering what would be the be best way to check if a column is a date <code>datatype</code>? Hopefully I explained it enough as I'm still fairly new.</p>\n<p>Cheers.</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 11213091, "reputation": 285, "user_id": 8226607, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/570bd360886ee39120488bf654c766f7?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Gayathri", "link": "https://stackoverflow.com/users/8226607/gayathri"}, "is_answered": true, "view_count": 4782, "accepted_answer_id": 69208225, "answer_count": 1, "score": 4, "last_activity_date": 1631794416, "creation_date": 1631739913, "last_edit_date": 1631741007, "question_id": 69199784, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69199784/dbt-post-hook-relation-my-table-does-not-exist", "title": "dbt post hook relation &quot;my_table&quot; does not exist", "body": "<p>I am building some models using dbt.</p>\n<p>I have a model so -</p>\n<pre><code>SELECT\n  COALESCE(\n    col1, col2\n  ) AS col,\n  ....\nFROM\n  {{ source(\n    'db',\n    'tbl'\n  ) }}\n  WHERE ....\n</code></pre>\n<p>This model has a config section calling a macro</p>\n<pre><code>{{- config(\n  post_hook = [macro()],\n  materialized='table'\n) -}}\n</code></pre>\n<p>Within the macro I use <code>{% if execute %}</code> and I also log to check the execute value <code>{{ log('Calling update macro with exec value = ' ~ execute) }}</code></p>\n<p>When I run <code>dbt compile</code> I do not expect the macro to fire according to the <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/execute\" rel=\"nofollow noreferrer\">documentation</a>. However, it does and actually sets the execute to true triggering the update and causing on error as the table doesn't exist. Am I missing something or is this a dbt bug? I am confused!</p>\n<p>Here's the line from the logs -\n<code>2021-09-15 20:48:16.864555 (Thread-1): Calling update macro with exec value = True</code></p>\n<p>.. and the error is\n<code> relation &quot;schema.my_table&quot; does not exist</code></p>\n<p>Appreciate any pointers someone might have, thanks</p>\n"}, {"tags": ["elasticsearch", "analytics", "business-intelligence", "dbt"], "owner": {"account_id": 427398, "reputation": 463, "user_id": 809141, "user_type": "registered", "accept_rate": 40, "profile_image": "https://www.gravatar.com/avatar/3dd873c4ee4279ed99f0e8f9ea697915?s=256&d=identicon&r=PG", "display_name": "denim", "link": "https://stackoverflow.com/users/809141/denim"}, "is_answered": true, "view_count": 1668, "answer_count": 2, "score": 4, "last_activity_date": 1645013994, "creation_date": 1589292123, "last_edit_date": 1595411901, "question_id": 61753739, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61753739/elastic-search-is-it-the-appropriate-db-for-an-analytics-team", "title": "Elastic Search - Is it the appropriate DB for an Analytics team?", "body": "<p>I am a member of an Analytics team that recently moved it's Data Warehouse into Elastic Search. The DW is accessed through Dremio.</p>\n\n<p>However, I am having second thoughts regarding whether Elastic Search is the appropriate DB for an Analytics team that performs a lot of day-to-day Analytics. I would prefer we kept our DW in one of BigQuery/Snowflake/Redshift and use \"dbt\" tool for transforming data and writing it back into the DB. </p>\n\n<p>I can't find a \"dbt\"-like tool to perform quick data transformations after reading from Elastic Search and Dremio is not mature enough tool for that. I would like to solicit your thoughts on Elastic Search and whether is an appropriate DB for day-to-day analytics.</p>\n\n<p>I appreciate your responses.</p>\n\n<p>Edit: \nI work at an online retailer. Our data is not \"big data\" in any sense. In the order of a few thousand orders per day. Most of our work is responding to inquiries from various teams/departments. Some of these questions go beyond a simple query. We have to build customized data marts that involve multiple steps in between. As a result, we need a tool that would allow us to transform data quickly and put the result set into a database. One such tool is \"dbt\" but it doesn't support Elastic Search. So the question is whether there is an appropriate tool for this job or Elastic Search is not appropriate for our use case.</p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 27636230, "reputation": 61, "user_id": 21094095, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/53297bb43e91d655292ae25517c0d5dd?s=256&d=identicon&r=PG", "display_name": "mxendire", "link": "https://stackoverflow.com/users/21094095/mxendire"}, "is_answered": true, "view_count": 8713, "accepted_answer_id": 75287675, "answer_count": 1, "score": 4, "last_activity_date": 1675111359, "creation_date": 1675092932, "last_edit_date": 1675111359, "question_id": 75286648, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75286648/multiple-profiles-in-one-profiles-yml-is-possible", "title": "multiple profiles in one profiles.yml is possible?", "body": "<p>dbt version: 1.3.1</p>\n<p>python version: 3.9.6</p>\n<p>adapter = dbt-synapse.yml</p>\n<pre><code># profiles.yml\n\ndefault: dbt_project\n\ndbt_project:\n  target: dev\n  outputs:\n    dev:\n      type: synapse  \n      driver: 'ODBC Driver 17 for SQL Server' \n      server: XXXXXXX\n      database: ###\n      port: 1433\n      schema: #######\n      user: ######\n      password: #####\n\n\nazure_blob:\n  target: dev\n  outputs:\n      dev:\n        type: azure_blob\n        account_name: ##\n        account_key: ##\n        container: ##\n        prefix: delta_lake\n\n</code></pre>\n<p><strong>--- after implied this change here is the error message a get--01/30/2023--- @2:32 pm central time----</strong></p>\n<p>i get this error when try to read the file from azure blob storage</p>\n<pre><code>-- the is the profiles.yml--------\n \ndefault: dbt_project\n\ndbt_project:\n  target: dev\n  outputs:\n    dev:\n      type: synapse  #synapse  #type: Azuresynapse\n      driver: 'ODBC Driver 17 for SQL Server' # (The ODBC Driver installed on your system)\n      server: XXXXXXX\n      database: XXXXXXX\n      port: 1433\n      schema: XXXXXXX\n      #authentication: sqlpassword\n      user: XXXXXXX\n      password: XXXXXXX\n    azure_blob:\n       type: azure_blob\n       account_name: XXXXXXX\n       account_key: XXXXXXX\n       container: data-platform-archive  #research-container/Bronze/Freedom/ABS_VESSEL/\n       prefix: abc/FGr1/fox/\n</code></pre>\n<p>--------------- dbt_project.yml-------------------------</p>\n<h1>name or the intended use of these models</h1>\n<pre><code>name: 'dbt_project'\nversion: '1.0.0'\nconfig-version: 2\n\n# This setting configures which &quot;profile&quot; dbt uses for this project.\nprofile: 'dbt_project'\n\nmodel-paths: [&quot;models&quot;]\nanalysis-paths: [&quot;analyses&quot;]\ntest-paths: [&quot;tests&quot;]\nseed-paths: [&quot;seeds&quot;]\nmacro-paths: [&quot;macros&quot;]\nsnapshot-paths: [&quot;snapshots&quot;]\n\ntarget-path: &quot;target&quot;  \nclean-targets:         \n  - &quot;target&quot;\n  - &quot;dbt_packages&quot;\n\n\nmodels:\n  dbt_project:\n     staging:\n      +materialized: table\n     utilities:\n      +materialized: view\n  azure_Blob:\n    staging:\n      +materialized: view   \n\n--------------------------------\n\nModel name=dbt_stg_DL_abs_acm_users.sql&quot;\nand here is the code\n\n{{ config(\n    materialized='view',\n    connection='azure_blob'\n) }}\n\nselect *\nfrom {{ source('data-platform-archive/abc/FGr1/fox/', 'abc.parquet') }}\n\n\n\nCompilation Error in model dbt_stg_DL_abs_acm_users\n  Model 'model.dbt_project.dbt_stg_DL_abs_acm_users' 'abc.parquet' which was not found\n</code></pre>\n"}, {"tags": ["dbt"], "owner": {"account_id": 9478316, "reputation": 616, "user_id": 7047037, "user_type": "registered", "profile_image": "https://graph.facebook.com/10207546524685823/picture?type=large", "display_name": "Catarina Ribeiro", "link": "https://stackoverflow.com/users/7047037/catarina-ribeiro"}, "is_answered": true, "view_count": 1963, "answer_count": 2, "score": 4, "last_activity_date": 1688957409, "creation_date": 1667490609, "question_id": 74305917, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/74305917/how-do-i-test-only-the-seeds-in-my-dbt-project", "title": "How do I test only the seeds in my dbt project?", "body": "<p>How can I test only the seeds folder on my dbt projet??\n<a href=\"https://i.sstatic.net/Zb2pt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Zb2pt.png\" alt=\"enter image description here\" /></a>\n<a href=\"https://i.sstatic.net/vDhWa.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/vDhWa.png\" alt=\"enter image description here\" /></a></p>\n<p>I've used <code>dbt test</code>, but it tests the entire project (seeds included). I've used <code>dbt test --select seeds</code> but it says: <code>Nothing to do. Try checking your model configs and model specification args</code></p>\n<p>Is there any way to test only the seeds folder?</p>\n<p>I've searched everywhere but I couldn't find anything regarding this</p>\n"}, {"tags": ["sql", "google-bigquery", "jinja2", "dbt"], "owner": {"account_id": 20340092, "reputation": 43, "user_id": 14919645, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/1febaf5c49d463b11dff2602ec8f87ca?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "jliebss", "link": "https://stackoverflow.com/users/14919645/jliebss"}, "is_answered": true, "view_count": 6230, "accepted_answer_id": 73254713, "answer_count": 1, "score": 4, "last_activity_date": 1659729342, "creation_date": 1659707826, "last_edit_date": 1659707981, "question_id": 73250933, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/73250933/how-do-i-run-sql-model-in-dbt-multiple-times-by-looping-through-variables", "title": "How do I run SQL model in dbt multiple times by looping through variables?", "body": "<p>I have a model in dbt (<strong>test_model</strong>) that accepts a geography variable (<em>zip</em>, <em>state</em>, <em>region</em>) in the configuration. I would like to run the model three times by looping through the variables, each time running it with a different variable.</p>\n<p>Here's the catch: I have a macro shown below that appends the variable to the end of the output table name (i.e., running <strong>test_model</strong> with <em>zip</em> as the variable outputs a table called <strong>test_model_zip</strong>). This is accomplished by adding <code>{{ config(alias=var('geo')) }}</code> at the top of the model.</p>\n<p>Whether I define the variable within dbt_project.yml, the model itself, or on the CLI, I've been unable to find a way to loop through these variables, each time passing the new variable to the configuration, and successfully create three tables. Do any of you have an idea how to accomplish this? FWIW, I'm using BigQuery SQL.</p>\n<p>The macro:</p>\n<pre><code>{% macro generate_alias_name(custom_alias_name=none, node=none) -%}\n\n    {%- if custom_alias_name is none -%}\n\n        {{ node.name }}\n\n    {%- else -%}\n\n        {% set node_name = node.name ~ '_' ~ custom_alias_name %}\n        {{ node_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n</code></pre>\n<p>The model, run by entering <code>dbt run --select test_model.sql --vars '{&quot;geo&quot;: &quot;zip&quot;}'</code> in the CLI:</p>\n<pre><code>{{ config(materialized='table', alias=var('geo')) }}\n\nwith query as (select 1 as id)\n\nselect * from query\n</code></pre>\n<p>The current output: a single table called <strong>test_model_zip</strong>.</p>\n<p>The desired output: three tables called <strong>test_model_zip</strong>, <strong>test_model_state</strong>, and <strong>test_model_region</strong>.</p>\n"}, {"tags": ["sql", "amazon-web-services", "amazon-redshift", "dbt"], "owner": {"account_id": 24112866, "reputation": 41, "user_id": 18083214, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJy0ogMc71XpcTx8fkQYWDu10ZScGTfSGYrqnWKf=k-s256", "display_name": "some_data_guy", "link": "https://stackoverflow.com/users/18083214/some-data-guy"}, "is_answered": true, "view_count": 1032, "answer_count": 1, "score": 4, "last_activity_date": 1643680410, "creation_date": 1643652365, "last_edit_date": 1643680410, "question_id": 70930745, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70930745/our-nightly-job-in-redshift-taking-10-12-hours-to-run", "title": "Our nightly job in Redshift taking 10-12 hours to run", "body": "<p>First post here.  Apologies if I leave anything out.  I work at a relatively small, &lt;70 person company.  We have a nightly job scheduled in DBT that runs in redshift.  This job is our attempt at a Last-Touch Attribution model.  This job is taking anywhere from 10-12 hours to run, and I just have to assume something is either wrong with our warehouse size or our modeling in DBT.</p>\n<p>We have 1 cluster (ra3.xlplus) in AWS and 3 nodes.  We are pulling roughly 100MM rows and I basically trying to see if anyone has ideas.  Our DBT models are materialized as incremental tables.  I have added additional queues to our redshift instance.  And that's as much as I've done.</p>\n<p>Happy to provide any additional detail if needed.</p>\n<p>Cheers all!</p>\n"}, {"tags": ["macros", "dbt"], "owner": {"account_id": 14696639, "reputation": 89, "user_id": 10613851, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/5c63a52e05556f1f41f2272ab10f7fe3?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Seba", "link": "https://stackoverflow.com/users/10613851/seba"}, "is_answered": true, "view_count": 8540, "accepted_answer_id": 63688501, "answer_count": 2, "score": 4, "last_activity_date": 1598966747, "creation_date": 1598954248, "last_edit_date": 1598954859, "question_id": 63685440, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63685440/usage-of-macros-in-dbt-project-config-version2", "title": "usage of macros in dbt_project config-version:2", "body": "<p>Our dbt_project.yml file, config-version: 1 has two variables across different incremental models, for which one we use a macro called <em>today()</em></p>\n<pre><code>vars:\n  start_date: '{{ today(offset_days=-1) }}'\n  end_date: '2999-12-31'\n</code></pre>\n<p>When testing the migration to dbt 0.17.2, config-version:2, we face the following issue</p>\n<pre><code>Running with dbt=0.17.2\nEncountered an error:\nCompilation Error\n  Could not render {{ today(offset_days=-1) }}: 'today' is undefined\n</code></pre>\n<p>The macro we built it end of last year. I believe dbt changed the way macro variables are referenced, however not sure how to tackle this.</p>\n<pre><code>-- returns current hour in YYYY-MM-DD-HH format,\n-- optionally offset by N days (-N or +N) or M hours (-M or +M)\n{%- macro current_hour(offset_days=0, offset_hours=0) -%}\n  {%- set now = modules.datetime.datetime.now(modules.pytz.utc) -%}\n  {%- set dt = now + modules.datetime.timedelta(days=offset_days, hours=offset_hours) -%}\n  {{- dt.strftime(&quot;%Y-%m-%d-%H&quot;) -}}\n{%- endmacro -%}\n\n-- returns current day in YYYY-MM-DD format,\n-- optionally offset by N days (-N or +N) or M hours (-M or +M)\n{%- macro today(offset_days=0, offset_hours=0) -%}\n  {{- current_hour(offset_days, offset_hours)[0:10] -}}\n{%- endmacro -%}\n\n-- accepts a timestamp string and returns a timestamo string\n-- formatted like 'YYYY-MM-DD HH24:MI:SS.US', e.g. '2019-11-02 06:11:42.690000'\n{%- macro dt_to_utc(ts_string) -%}\n  TO_CHAR({{ ts_string }}::TIMESTAMPTZ, 'YYYY-MM-DD HH24:MI:SS.US')\n{%- endmacro -%}\n</code></pre>\n"}, {"tags": ["dbt"], "owner": {"account_id": 14567808, "reputation": 11380, "user_id": 10521959, "user_type": "registered", "profile_image": "https://i.sstatic.net/DymiN.png?s=256", "display_name": "Yaakov Bressler", "link": "https://stackoverflow.com/users/10521959/yaakov-bressler"}, "is_answered": true, "view_count": 1116, "accepted_answer_id": 77084777, "answer_count": 2, "score": 4, "last_activity_date": 1718967091, "creation_date": 1694453323, "question_id": 77083700, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/77083700/disable-elementary-from-dbt-cli-when-running-dbt-test", "title": "Disable elementary from DBT CLI when running dbt test", "body": "<p>I have <a href=\"https://docs.elementary-data.com/understand-elementary/elementary-overview\" rel=\"nofollow noreferrer\">Elementary</a> installed in my DBT project:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code># packages.yml\npackages:\n  - package: elementary-data/elementary\n    version: 0.9.0\n\n# dbt_project.yml\nmodels:\n  elementary:\n    +schema: elementary\n</code></pre>\n<p>When I run DBT from the CLI, elementary will open connections and model executions, as well as test executions + results. <em><strong>I'd like to disable elementary's behavior, from the CLI.</strong></em> <em>(Not sure if it's possible...)</em></p>\n<p>An ideal solution would be a CLI flag:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>dbt run -s my_model --disable-elementary\n</code></pre>\n<p><em>(I do not want to alter my profiles... or dbt_project.yml)</em></p>\n"}, {"tags": ["dbt"], "owner": {"account_id": 25756866, "reputation": 41, "user_id": 19508386, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/43a670cec40549845577ddd308b5ad4d?s=256&d=identicon&r=PG", "display_name": "annie679", "link": "https://stackoverflow.com/users/19508386/annie679"}, "is_answered": true, "view_count": 12484, "answer_count": 1, "score": 4, "last_activity_date": 1657471797, "creation_date": 1657264055, "question_id": 72907860, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/72907860/creation-of-table-using-dbt", "title": "Creation of Table using DBT", "body": "<p>Can we create a new table in DBT?\nCan we copy the table structure which is present in the dev environment in the database to another environment using DBT?</p>\n"}, {"tags": ["sql-server", "dbt"], "owner": {"account_id": 22968572, "reputation": 69, "user_id": 17097294, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/268970fe2b0450488532652c6a5785d6?s=256&d=identicon&r=PG", "display_name": "Devom ", "link": "https://stackoverflow.com/users/17097294/devom"}, "is_answered": true, "view_count": 2374, "accepted_answer_id": 71117383, "answer_count": 3, "score": 4, "last_activity_date": 1652263499, "creation_date": 1643748292, "question_id": 70947285, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/70947285/using-variable-arrays-in-models", "title": "Using variable arrays in models", "body": "<p>Is it possible to define an array in the vars section and use it inside the SQL syntax of a model?</p>\n<p>Something like this</p>\n<p><strong>dbt_project.yml:</strong></p>\n<pre><code>vars:\n  active_country_codes: ['it','ge']\n</code></pre>\n<p><strong>model.sql</strong></p>\n<pre><code>SELECT ... \n  FROM TABLE WHERE country_code IN ('{{ var(&quot;active_country_codes&quot;) }}')\n</code></pre>\n<p>I've tried with a single value, i.e:<strong>['it']</strong>, and works but if I add another it starts failing.</p>\n<p>I am using the SQL Server Data connector.</p>\n"}, {"tags": ["apache-spark", "dbt"], "owner": {"account_id": 127266, "reputation": 9063, "user_id": 324315, "user_type": "registered", "accept_rate": 66, "profile_image": "https://www.gravatar.com/avatar/0ec29bf8d52efa4aa8a3f5d9c63fc464?s=256&d=identicon&r=PG&f=y&so-version=2", "display_name": "Randomize", "link": "https://stackoverflow.com/users/324315/randomize"}, "is_answered": true, "view_count": 1355, "accepted_answer_id": 67970302, "answer_count": 1, "score": 4, "last_activity_date": 1623674047, "creation_date": 1623399313, "question_id": 67933530, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67933530/how-to-pass-spark-configuration-parameters-to-dbt", "title": "How to pass Spark configuration parameters to DBT?", "body": "<p>I am using DBT to connect to AWS/EMR. I am able to run Spark/SQL queries but where do I set parameters like for example <code>spark.sql.shuffle.partitions</code>, that in normal code you will pass with:</p>\n<pre><code>sqlContext.setConf(&quot;spark.sql.shuffle.partitions&quot;, &quot;1200&quot;)\n</code></pre>\n<p>?</p>\n"}, {"tags": ["dbt", "dbtype"], "owner": {"account_id": 26449163, "reputation": 145, "user_id": 20092606, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/fb4c990da7ea2cddc7f0b0672de28ca9?s=256&d=identicon&r=PG", "display_name": "Harris", "link": "https://stackoverflow.com/users/20092606/harris"}, "is_answered": true, "view_count": 534, "accepted_answer_id": 75230545, "answer_count": 1, "score": 4, "last_activity_date": 1674629492, "creation_date": 1674618431, "question_id": 75229533, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/75229533/dbt-age-validation-using-conditional-statement", "title": "DBT age validation using conditional statement", "body": "<p>Hello I would like to ask for your assistance I'm planning to validate my age field on database however I'm not sure how to make it work on DBT. Your response is highly appreciated. Thank you</p>\n<pre><code>  - name: age\n    tests:\n      - not_null\n  #     - dbt_expectations.expect_column_values_to_match_regex:\n  #         regex: &quot;[1-9]\\\\d{3,}&quot;\n  #     - dbt_utils.not_empty_string\n  #     - dbt_utils.not_constant\n      - dbt_utils.expression_is_true:\n        expression: &quot;age&quot;\n        config:\n              where: &quot;age &gt; '110'&quot;\n</code></pre>\n<p><a href=\"https://i.sstatic.net/FSup5.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/FSup5.png\" alt=\"enter image description here\" /></a></p>\n<p>I tried to use this syntax below however its not working and the complied statement showed.</p>\n<pre><code>  - name: age\n    tests:\n      - not_null\n  #     - dbt_expectations.expect_column_values_to_match_regex:\n  #         regex: &quot;[1-9]\\\\d{3,}&quot;\n  #     - dbt_utils.not_empty_string\n  #     - dbt_utils.not_constant\n      - dbt_utils.expression_is_true:\n          expression: '&gt;= 110'\n</code></pre>\n<p>Result:</p>\n<p><a href=\"https://i.sstatic.net/1uFbK.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/1uFbK.png\" alt=\"enter image description here\" /></a></p>\n"}], "has_more": true, "quota_max": 300, "quota_remaining": 296}